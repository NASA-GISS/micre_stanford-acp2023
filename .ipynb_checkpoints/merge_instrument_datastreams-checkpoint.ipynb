{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d82a81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#=========================================================\n",
    "# merge_insturments.py\n",
    "\n",
    "# Date Created: 04/15/2022\n",
    "# Date Updated: 04/15/2022\n",
    "#========================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cdab11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------\n",
    "# Imports\n",
    "#--------------------------------\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import xarray\n",
    "import datetime\n",
    "import calendar\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from file_struct import file_struct as fs\n",
    "from load_sonde_data import load_sonde_data\n",
    "from give_me_files_and_subfolders import give_me_files_and_subfolders\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from scipy.interpolate import NearestNDInterpolator as nn\n",
    "from scipy.interpolate import griddata as griddata\n",
    "from calculate_theta_and_more import calculate_theta_and_more\n",
    "import pandas\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.units import units\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.lines import Line2D\n",
    "from scipy.interpolate import interp2d\n",
    "from scipy.interpolate import griddata\n",
    "#import act\n",
    "import metpy\n",
    "import metpy.calc as mpcalc\n",
    "from metpy.cbook import get_test_data\n",
    "from metpy.plots import add_metpy_logo, SkewT\n",
    "from metpy.units import units\n",
    "import time\n",
    "from dask.distributed import Client, progress, LocalCluster\n",
    "import csv\n",
    "#import seaborn as sns\n",
    "#--------------------------------------------\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c41b259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "# Functions\n",
    "#--------------------------------------------\n",
    "def toTimestamp(d):\n",
    "    return calendar.timegm(d.timetuple())\n",
    "\n",
    "def toDatetime(d):\n",
    "    return datetime.datetime.utcfromtimestamp(d)\n",
    "\n",
    "def find_nearest(array, value):\n",
    "    array = np.asarray(array)\n",
    "    idx = (np.abs(array - value)).argmin()\n",
    "    return array[idx],idx   \n",
    "\n",
    "# function to make serial date numbers which are the number of days that have passed\n",
    "# since epoch beginning given as days.fraction_of_day\n",
    "def datenum(d):\n",
    "        return 366 + d.toordinal() + (d - datetime.datetime.fromordinal(d.toordinal())).total_seconds()/(24*60*60)\n",
    "#--------------------------------------------\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e927be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "# Grab BASTA files\n",
    "#==================================================\n",
    "basta_path = '/mnt/raid/mwstanfo/micre_basta/BASTA_25m/'\n",
    "basta_files = glob.glob(basta_path+'*.nc')\n",
    "basta_files = sorted(basta_files)\n",
    "basta_files = np.array(basta_files)\n",
    "\n",
    "basta_dates_dt = []\n",
    "for ii in range(len(basta_files)):\n",
    "    fname = basta_files[ii]\n",
    "    tmp_str = fname.split('_')\n",
    "    tmp_str = tmp_str[-1]\n",
    "    tmp_str = tmp_str.split('.')\n",
    "    tmp_str = tmp_str[0]\n",
    "    tmp_year = int(tmp_str[0:4])\n",
    "    tmp_month = int(tmp_str[4:6])\n",
    "    tmp_day = int(tmp_str[6:8])\n",
    "    basta_dates_dt.append(datetime.datetime(tmp_year,tmp_month,tmp_day,0,0,0))\n",
    "basta_dates_dt = np.array(basta_dates_dt)\n",
    "dates = basta_dates_dt.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0f9857e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "# Grab ARM Ceilometer files\n",
    "#==================================================\n",
    "arm_ceil_path = '/mnt/raid/mwstanfo/micre_ceil/'\n",
    "arm_ceil_files = glob.glob(arm_ceil_path+'*.nc')\n",
    "arm_ceil_files = sorted(arm_ceil_files)\n",
    "arm_ceil_files = np.array(arm_ceil_files)\n",
    "\n",
    "arm_ceil_dates_dt = []\n",
    "for ii in range(len(arm_ceil_files)):\n",
    "    fname = arm_ceil_files[ii]\n",
    "    tmp_str = fname.split('/')\n",
    "    tmp_str = tmp_str[-1]\n",
    "    tmp_str = tmp_str.split('.')\n",
    "    tmp_str = tmp_str[2]\n",
    "    tmp_year = int(tmp_str[0:4])\n",
    "    tmp_month = int(tmp_str[4:6])\n",
    "    tmp_day = int(tmp_str[6:8])\n",
    "    arm_ceil_dates_dt.append(datetime.datetime(tmp_year,tmp_month,tmp_day,0,0,0))\n",
    "arm_ceil_dates_dt = np.array(arm_ceil_dates_dt)\n",
    "\n",
    "# Limit ceilometer files to encompass only BASTA dates\n",
    "tmpid = np.where((arm_ceil_dates_dt >= basta_dates_dt[0]) & (arm_ceil_dates_dt <= basta_dates_dt[-1]))[0]\n",
    "arm_ceil_files = arm_ceil_files[tmpid]\n",
    "arm_ceil_dates_dt = arm_ceil_dates_dt[tmpid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e637ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "# Grab University of Canterbury Ceilometer files\n",
    "#==================================================\n",
    "aad_ceil_path = '/mnt/raid/mwstanfo/aad_ceil/merged/'\n",
    "aad_ceil_files = glob.glob(aad_ceil_path+'*.nc')\n",
    "aad_ceil_files = sorted(aad_ceil_files)\n",
    "aad_ceil_files = np.array(aad_ceil_files)\n",
    "\n",
    "\n",
    "aad_ceil_dates_dt = []\n",
    "for ii in range(len(aad_ceil_files)):\n",
    "    fname = aad_ceil_files[ii]\n",
    "    tmp_str = fname.split('/')\n",
    "    tmp_str = tmp_str[-1]\n",
    "    tmp_str = tmp_str.split('.')\n",
    "    tmp_str = tmp_str[0]\n",
    "    tmp_str = tmp_str.split('_')\n",
    "    tmp_year = int(tmp_str[3])\n",
    "    tmp_month = int(tmp_str[4])\n",
    "    tmp_day = int(tmp_str[5])\n",
    "    aad_ceil_dates_dt.append(datetime.datetime(tmp_year,tmp_month,tmp_day))\n",
    "    \n",
    "aad_ceil_dates_dt = np.array(aad_ceil_dates_dt)\n",
    "# Limit ceilometer files to encompass only BASTA dates\n",
    "tmpid = np.where((aad_ceil_dates_dt >= basta_dates_dt[0]) & (aad_ceil_dates_dt <= basta_dates_dt[-1]))[0]\n",
    "aad_ceil_dates_dt = aad_ceil_dates_dt[tmpid] \n",
    "aad_ceil_files = aad_ceil_files[tmpid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02af9ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "# Grab surface meteorology files\n",
    "#==================================================\n",
    "sfc_path = '/mnt/raid/mwstanfo/micre_sfc/'\n",
    "sfc_files = glob.glob(sfc_path+'*.nc')\n",
    "sfc_files = sorted(sfc_files)\n",
    "sfc_files = np.array(sfc_files)\n",
    "\n",
    "sfc_dates_dt = []\n",
    "for ii in range(len(sfc_files)):\n",
    "    fname = sfc_files[ii]\n",
    "    tmp_str = fname.split('/')\n",
    "    tmp_str = tmp_str[-1]\n",
    "    tmp_str = tmp_str.split('.')\n",
    "    tmp_str = tmp_str[2]\n",
    "    tmp_year = int(tmp_str[0:4])\n",
    "    tmp_month = int(tmp_str[4:6])\n",
    "    tmp_day = int(tmp_str[6:8])\n",
    "    sfc_dates_dt.append(datetime.datetime(tmp_year,tmp_month,tmp_day,0,0,0))\n",
    "sfc_dates_dt = np.array(sfc_dates_dt)\n",
    "\n",
    "# Limit sfc met files to encompass only BASTA dates\n",
    "tmpid = np.where((sfc_dates_dt >= basta_dates_dt[0]) & (sfc_dates_dt <= basta_dates_dt[-1]))[0]\n",
    "sfc_dates_dt = sfc_dates_dt[tmpid] \n",
    "sfc_files = sfc_files[tmpid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db4f4abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "# Grab satellite files\n",
    "#==================================================\n",
    "sat_path = '/mnt/raid/mwstanfo/post_process_files/visst_gridded/'\n",
    "sat_files = glob.glob(sat_path+'*.nc')\n",
    "sat_files = sorted(sat_files)\n",
    "sat_files = np.array(sat_files)\n",
    "\n",
    "sat_dates_dt = []\n",
    "for ii in range(len(sat_files)):\n",
    "    fname = sat_files[ii]\n",
    "    tmp_str = fname.split('/')\n",
    "    tmp_str = tmp_str[-1]\n",
    "    tmp_str = tmp_str.split('.')\n",
    "    tmp_str = tmp_str[0]\n",
    "    tmp_str = tmp_str.split('_')\n",
    "    tmp_str = tmp_str[2:]\n",
    "    tmp_year = int(tmp_str[0])\n",
    "    tmp_month = int(tmp_str[1])\n",
    "    tmp_day = int(tmp_str[2])\n",
    "    sat_dates_dt.append(datetime.datetime(tmp_year,tmp_month,tmp_day,0,0,0))\n",
    "sat_dates_dt = np.array(sat_dates_dt)\n",
    "\n",
    "# sort files according to dates\n",
    "sort_id = np.argsort(sat_dates_dt)\n",
    "sat_dates_dt = sat_dates_dt[sort_id]\n",
    "sat_files = sat_files[sort_id]\n",
    "\n",
    "# Limit sat files to encompass only BASTA dates\n",
    "tmpid = np.where((sat_dates_dt >= basta_dates_dt[0]) & (sat_dates_dt <= basta_dates_dt[-1]))[0]\n",
    "sat_dates_dt = sat_dates_dt[tmpid] \n",
    "sat_files = sat_files[tmpid]\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ce6715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "# Grab optics files\n",
    "#==================================================\n",
    "path = '/mnt/raid/mwstanfo/micre_optics/'\n",
    "optics_files = glob.glob(path+'*.cdf')\n",
    "optics_files = sorted(optics_files)\n",
    "optics_files = np.array(optics_files)\n",
    "nf = len(optics_files)\n",
    "#print(optics_files)\n",
    "\n",
    "optics_dates_dt = []\n",
    "for ff in range(nf):\n",
    "    optics_file = optics_files[ff]\n",
    "    optics_file = str.split(optics_file,'/')[-1]\n",
    "    optics_file = str.split(optics_file,'.')[-3]\n",
    "    year = int(optics_file[0:4])\n",
    "    month = int(optics_file[4:6])\n",
    "    day = int(optics_file[6:8])\n",
    "    optics_date_dt = datetime.datetime(year,month,day)\n",
    "    optics_dates_dt.append(optics_date_dt)\n",
    "optics_dates_dt = np.array(optics_dates_dt)\n",
    "# limit to only basta times\n",
    "tmpid = np.where((optics_dates_dt >= basta_dates_dt[0]) & (optics_dates_dt <= basta_dates_dt[-1]))[0]\n",
    "optics_files = optics_files[tmpid]\n",
    "optics_dates_dt = optics_dates_dt[tmpid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "581c7608",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "#if True:\n",
    "    sns.set_theme()\n",
    "    #sns.set_style('dark')\n",
    "    sns.set_style('ticks')\n",
    "    sns.set(rc={'axes.facecolor':'lavender','axes.edgecolor': 'black','grid.color':'white'})\n",
    "    sns.set_context('talk') \n",
    "\n",
    "    # Look at optics files\n",
    "    ii=0\n",
    "    for file in optics_files[10:]:\n",
    "        print(file)\n",
    "        ncfile = xarray.open_dataset(file,decode_times=False)\n",
    "        base_time = ncfile['base_time'].values\n",
    "        time_offset = ncfile['time_offset'].values\n",
    "        qc_time = ncfile['qc_time'].values\n",
    "        tau_inst = ncfile['optical_depth_instantaneous'].values\n",
    "        qc_tau_inst = ncfile['qc_optical_depth_instantaneous'].values\n",
    "        reff_inst = ncfile['effective_radius_instantaneous'].values\n",
    "        qc_reff_inst = ncfile['qc_effective_radius_instantaneous'].values\n",
    "        tau_avg = ncfile['optical_depth_instantaneous'].values\n",
    "        qc_tau_avg = ncfile['qc_optical_depth_average'].values\n",
    "        reff_avg = ncfile['effective_radius_average'].values\n",
    "        qc_reff_avg = ncfile['qc_effective_radius_average'].values\n",
    "        trans1 = ncfile['total_transmittance_filter1'].values\n",
    "        trans2 = ncfile['total_transmittance_filter2'].values\n",
    "        trans3 = ncfile['total_transmittance_filter3'].values\n",
    "        trans4 = ncfile['total_transmittance_filter4'].values\n",
    "        trans5 = ncfile['total_transmittance_filter5'].values\n",
    "        lwp = ncfile['lwp'].values\n",
    "        qc_lwp = ncfile['qc_lwp'].values\n",
    "        pwv = ncfile['pwv'].values\n",
    "        qc_pwv = ncfile['qc_pwv'].values\n",
    "        cf = ncfile['cloudfraction'].values\n",
    "        qc_cf = ncfile['qc_cloudfraction'].values\n",
    "        ncfile.close()\n",
    "        time_ts = base_time + time_offset\n",
    "        time_dt = np.array([toDatetime(time_ts[dd]) for dd in range(len(time_ts))])\n",
    "\n",
    "              \n",
    "\n",
    "        fig = plt.figure(figsize=(20,14))\n",
    "        ax_tau = fig.add_subplot(321)\n",
    "        ax_reff = fig.add_subplot(322)\n",
    "        ax_lwp = fig.add_subplot(323)\n",
    "        ax_cf = fig.add_subplot(324)\n",
    "        ax_trans = fig.add_subplot(325)\n",
    "        Fontsize=16\n",
    "        dfmt = mdates.DateFormatter('%H:%M')\n",
    "\n",
    "        ax_list = [ax_tau,ax_reff,ax_lwp,ax_cf,ax_trans]\n",
    "        for ax in ax_list:\n",
    "            ax.grid(True)\n",
    "            ax.tick_params(labelsize=Fontsize)\n",
    "            ax.xaxis.set_major_formatter(dfmt)\n",
    "            ax.set_xlabel('UTC Time [HH:MM]',fontsize=Fontsize)\n",
    "            ax.set_axisbelow(True)\n",
    "\n",
    "        ax_tau.set_title('Optical Depth ($\\\\tau$)',fontsize=Fontsize*1.5,fontweight='bold')\n",
    "        ax_tau.set_ylabel('$\\\\tau$',fontsize=Fontsize)\n",
    "        ax_reff.set_title('Effective Radius ($R_{eff}$)',fontsize=Fontsize*1.5,fontweight='bold')\n",
    "        ax_reff.set_ylabel('$R_{eff}$ [$\\\\mu$m]',fontsize=Fontsize)\n",
    "        ax_lwp.set_title('Liquid Water Path (LWP)',fontsize=Fontsize*1.5,fontweight='bold')\n",
    "        ax_lwp.set_ylabel('LWP [g m$^{-2}$]',fontsize=Fontsize)\n",
    "        ax_cf.set_title('Cloud Fraction',fontsize=Fontsize*1.5,fontweight='bold')\n",
    "        ax_cf.set_ylabel('Cloud Fraction',fontsize=Fontsize)\n",
    "        ax_tau.set_yscale('log')\n",
    "        ax_lwp.set_yscale('log')\n",
    "        ax_trans.set_title('Total transmittance of Narrowband\\nHemispheric Irradiance',fontsize=Fontsize,fontweight='bold')\n",
    "        ax_trans.set_ylabel('Transmittance',fontsize=Fontsize)\n",
    "        ax_cf.set_ylabel('Cloud Fraction',fontsize=Fontsize)\n",
    "        \n",
    "        # Plot\n",
    "        ax_tau.plot(time_dt,tau_inst,label='$\\\\tau_{inst}$',c='black',lw=2)\n",
    "        ax_tau.plot(time_dt,tau_avg,label='$\\\\tau_{avg}$',c='red',lw=2,ls='dashed')\n",
    "        ax_tau.legend(loc='upper left',fontsize=Fontsize)\n",
    "        ax_reff.plot(time_dt,reff_inst,label='$R_{eff,inst}$',c='black',lw=2)\n",
    "        ax_reff.plot(time_dt,reff_avg,label='$R_{eff,avg}$',c='red',lw=2,ls='dashed')\n",
    "        ax_reff.legend(loc='upper left',fontsize=Fontsize)\n",
    "        ax_lwp.plot(time_dt,lwp,c='black',lw=2)\n",
    "        ax_cf.plot(time_dt,cf,c='black',lw=2)\n",
    "        ax_trans.plot(time_dt,trans1,c='black',lw=1,label='Filter 1')\n",
    "        ax_trans.plot(time_dt,trans1,c='red',lw=1,label='Filter 2')\n",
    "        ax_trans.plot(time_dt,trans3,c='darkorange',lw=1,label='Filter 3')\n",
    "        ax_trans.plot(time_dt,trans4,c='blue',lw=1,label='Filter 4')\n",
    "        ax_trans.plot(time_dt,trans5,c='deepskyblue',lw=1,label='Filter 5')\n",
    "        ax_trans.legend(loc='upper center',fontsize=Fontsize,ncol=2)\n",
    "\n",
    "        dumdate = datetime.datetime(time_dt[0].year,time_dt[0].month,time_dt[0].day)\n",
    "        dumdate = dumdate.strftime('%Y/%m/%d')\n",
    "        plt.suptitle(dumdate,fontsize=Fontsize*2.,fontweight='bold')\n",
    "        plt.subplots_adjust(hspace=0.4)\n",
    "\n",
    "        plt.show()\n",
    "        print(aaaa)\n",
    "\n",
    "        dumdate = datetime.datetime(time_dt[0].year,time_dt[0].month,time_dt[0].day)\n",
    "        dumdate = dumdate.strftime('%Y%m%d')\n",
    "        fig_path = '/home/mwstanfo/figures/micre_v2/optics/'\n",
    "        outfile = 'optics_{}.png'.format(dumdate)\n",
    "        plt.savefig(fig_path+outfile,dpi=200,bbox_inches='tight')       \n",
    "        plt.close()    \n",
    "        ii+=1\n",
    "\n",
    "        #print(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f277ad7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "# Grab sounding files\n",
    "#==================================================\n",
    "path = '/mnt/raid/mwstanfo/micre_soundings/'\n",
    "sonde_files = glob.glob(path+'*.nc')\n",
    "sonde_files = sorted(sonde_files)\n",
    "sonde_files = np.array(sonde_files)\n",
    "nf = len(sonde_files)\n",
    "\n",
    "sonde_dates_dt = []\n",
    "sonde_times_dt = []\n",
    "for ff in range(nf):\n",
    "    sonde_file = sonde_files[ff]\n",
    "    sonde_file = str.split(sonde_file,'/')[-1]\n",
    "    sonde_file = str.split(sonde_file,'.')[0]\n",
    "    sonde_file_str1 = str.split(sonde_file,'_')[0]\n",
    "    sonde_file_str2 = str.split(sonde_file,'_')[1]\n",
    "    year = int(sonde_file_str1[0:4])\n",
    "    month = int(sonde_file_str1[4:6])\n",
    "    day = int(sonde_file_str1[6:8])\n",
    "    hour = int(sonde_file_str2[0:2])\n",
    "    minute = int(sonde_file_str2[2:4])\n",
    "    sonde_time_dt = datetime.datetime(year,month,day,hour,minute)\n",
    "    sonde_date_dt = datetime.datetime(year,month,day)\n",
    "    sonde_dates_dt.append(sonde_date_dt)\n",
    "    sonde_times_dt.append(sonde_time_dt)\n",
    "sonde_dates_dt = np.array(sonde_dates_dt)\n",
    "sonde_times_dt = np.array(sonde_times_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e13f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "# Grab cluster files\n",
    "#==================================================\n",
    "cluster_file ='/home/mwstanfo/cluster_information_all.xlsx'\n",
    "\n",
    "clusters = pd.read_excel(cluster_file)\n",
    "clusters = np.array(clusters)\n",
    "cluster_number = clusters[:,0]\n",
    "cluster_dates = clusters[:,1]\n",
    "cluster_ids = clusters[:,2]\n",
    "\n",
    "cluster_dates_dt = []\n",
    "cluster_times_dt = []\n",
    "for cluster_date in cluster_dates:\n",
    "    tmpstr1,tmpstr2 = str.split(cluster_date,'_')\n",
    "    tmpyear = int(tmpstr1[0:4])\n",
    "    tmpmonth = int(tmpstr1[4:6])\n",
    "    tmpday = int(tmpstr1[6:8])\n",
    "    tmphour = int(tmpstr2[0:2])\n",
    "    tmpminute = int(tmpstr2[2:4])\n",
    "    cluster_times_dt.append(datetime.datetime(tmpyear,tmpmonth,tmpday,tmphour,tmpminute))\n",
    "    cluster_dates_dt.append(datetime.datetime(tmpyear,tmpmonth,tmpday))\n",
    "cluster_dates_dt = np.array(cluster_dates_dt)\n",
    "cluster_times_dt = np.array(cluster_times_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb0087ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "# Grab disdrometer (PIRAT) files\n",
    "#==================================================\n",
    "dis_path = '/mnt/raid/mwstanfo/micre_disdrometer/PIRAT_rates_csv/'\n",
    "dis_files = glob.glob(dis_path+'*.csv')\n",
    "dis_files = sorted(dis_files)\n",
    "dis_files = np.array(dis_files)\n",
    "dis_dates_dt = []\n",
    "\n",
    "for ii in range(len(dis_files)):\n",
    "    tmp_str = dis_files[ii].split('/')\n",
    "    tmp_str = tmp_str[-1]\n",
    "    tmp_str = tmp_str.split('_')\n",
    "    tmp_str = tmp_str[1]\n",
    "    tmp_str = tmp_str.split('.')\n",
    "    tmp_str = tmp_str[0]\n",
    "    tmp_year = int(tmp_str[0:4])\n",
    "    tmp_month = int(tmp_str[4:6])\n",
    "    tmp_day = int(tmp_str[6:8])\n",
    "    dis_dates_dt.append(datetime.datetime(tmp_year,tmp_month,tmp_day))\n",
    "    #dis_data = pd.read_csv(dis_files[ii],delimiter=',',header=0)\n",
    "    #dis_data = np.array(dis_data)\n",
    "    #dis_time = dis_data[:,0] # time in fraction of UTC hour\n",
    "    #dis_DQ = dis_data[:,1] # 0: good, != 0: bad\n",
    "    #dis_R = dis_data[:,2] # precipitation rate in mm/hr\n",
    "dis_dates_dt = np.array(dis_dates_dt)\n",
    "\n",
    "# Limit disdrometer files to encompass only BASTA dates\n",
    "tmpid = np.where((dis_dates_dt >= basta_dates_dt[0]) & (dis_dates_dt <= basta_dates_dt[-1]))[0]\n",
    "dis_dates_dt = dis_dates_dt[tmpid]\n",
    "dis_files = dis_files[tmpid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "154aea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dis(date,dis_dates_dt,dis_files,avg_bool,basta_time_ts):\n",
    "    tmpid = np.where(dis_dates_dt == date)\n",
    "    if np.size(tmpid) == 0.:\n",
    "        dis_present_flag = False\n",
    "        dis_dict = None\n",
    "        return dis_present_flag,dis_dict\n",
    "    elif np.size(tmpid) > 0.:\n",
    "        dis_present_flag = True\n",
    "        tmpid = tmpid[0][0]\n",
    "        current_dis_file = dis_files[tmpid]\n",
    "        current_dis_date = dis_dates_dt[tmpid]\n",
    "\n",
    "        dis_data = pd.read_csv(current_dis_file,delimiter=',',header=0)\n",
    "        dis_data = np.array(dis_data)\n",
    "        dis_time = dis_data[:,0] # time in fraction of UTC hour\n",
    "        dis_dq = dis_data[:,1] # 0: good, != 0: bad\n",
    "        dis_precip_rate = dis_data[:,2] # precipitation rate in mm/hr\n",
    "\n",
    "        # convert dis_time from fraction of UTC hour to datetime object and timestamp\n",
    "        dis_date = current_dis_date\n",
    "        dis_year = dis_date.year\n",
    "        dis_month = dis_date.month\n",
    "        dis_day = dis_date.day\n",
    "        dis_hour = np.array([np.floor(dis_time[dd]) for dd in range(len(dis_time))])\n",
    "        fraction_of_hour = np.array([(dis_time[dd]-dis_hour[dd]) for dd in range(len(dis_time))])\n",
    "        dis_minute = fraction_of_hour*60.\n",
    "\n",
    "        dis_time_dt = np.array([datetime.datetime(dis_year,\\\n",
    "                                                  dis_month,\\\n",
    "                                                  dis_day,\\\n",
    "                                                  int(dis_hour[dd]),\\\n",
    "                                                  int(dis_minute[dd])) for dd in range(len(dis_time))])\n",
    "        dis_time_ts = np.array([toTimestamp(dis_time_dt[dd]) for dd in range(len(dis_time))])\n",
    "\n",
    "        # NaN out bad values\n",
    "        dis_precip_rate[dis_dq > 0.] = np.nan\n",
    "        \n",
    "        dis_dict = {'precip_rate':dis_precip_rate,'time_dt':dis_time_dt,'time_ts':dis_time_ts} \n",
    "        return dis_present_flag, dis_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63b315cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sfc_met(date,sfc_dates_dt,sfc_files,avg_bool,basta_time_ts):\n",
    "\n",
    "    tmpid = np.where(sfc_dates_dt == date)\n",
    "    if np.size(tmpid) == 0.:\n",
    "        sfc_met_present_flag = False\n",
    "        sfc_met_out_dict = None\n",
    "        return sfc_met_present_flag, sfc_met_out_dict\n",
    "    elif np.size(tmpid) > 0.:        \n",
    "        sfc_met_present_flag = True\n",
    "        tmpid = tmpid[0][0]\n",
    "        current_sfc_file = sfc_files[tmpid]\n",
    "        \n",
    "        ncfile = xarray.open_dataset(current_sfc_file,decode_times=False)\n",
    "        sfc_dims = ncfile.dims\n",
    "        sfc_base_time = np.array(ncfile['base_time'].copy())\n",
    "        sfc_num_times = sfc_dims['time']\n",
    "        sfc_time_offset = np.array(ncfile['time_offset'].copy())\n",
    "        sfc_temperature = np.array(ncfile['temperature'].copy())\n",
    "        sfc_pressure = np.array(ncfile['pressure'].copy())\n",
    "        sfc_rh = np.array(ncfile['relative_humidity'].copy())\n",
    "        sfc_wind_speed = np.array(ncfile['wind_speed'].copy())\n",
    "        sfc_wind_dir = np.array(ncfile['wind_direction'].copy())\n",
    "        sfc_precip = np.array(ncfile['cumulative_precipitation'].copy())\n",
    "        ncfile.close()\n",
    "\n",
    "        sfc_time_ts = [int(sfc_base_time + sfc_time_offset[dd]) for dd in range(sfc_num_times)]\n",
    "        sfc_time_dt = [toDatetime(sfc_time_ts[dd]) for dd in range(sfc_num_times)]\n",
    "\n",
    "        # NaN out missing values\n",
    "        # original missing value is -9999.\n",
    "        sfc_temperature[sfc_temperature < -950.] = np.nan\n",
    "        sfc_pressure[sfc_pressure < -950.] = np.nan\n",
    "        sfc_rh[sfc_rh < -950.] = np.nan\n",
    "        sfc_wind_speed[sfc_wind_speed < -950.] = np.nan\n",
    "        sfc_wind_dir[sfc_wind_dir < -950.] = np.nan\n",
    "        sfc_precip[sfc_precip < -950.] = np.nan\n",
    "        \n",
    "        # interpolate sfc variables to basta time grid\n",
    "        sfc_temperature_interp = np.interp(basta_time_ts,sfc_time_ts,sfc_temperature)\n",
    "        sfc_rh_interp = np.interp(basta_time_ts,sfc_time_ts,sfc_rh)\n",
    "        sfc_wind_speed_interp = np.interp(basta_time_ts,sfc_time_ts,sfc_wind_speed)\n",
    "        sfc_wind_dir_interp = np.interp(basta_time_ts,sfc_time_ts,sfc_wind_dir,period=360)\n",
    "        sfc_pressure_interp = np.interp(basta_time_ts,sfc_time_ts,sfc_pressure)\n",
    "\n",
    "\n",
    "        sfc_met_out_dict = {'native_temperature':sfc_temperature,\\\n",
    "                    'native_rh':sfc_rh,\\\n",
    "                    'native_pressure':sfc_pressure,\\\n",
    "                    'native_wind_speed':sfc_wind_speed,\\\n",
    "                    'native_wind_dir':sfc_wind_dir,\\\n",
    "                    'native_precip':sfc_precip,\\\n",
    "                    'native_time_dt':sfc_time_dt,\\\n",
    "                    'native_time_ts':sfc_time_ts,\\\n",
    "                    'temperature':sfc_temperature_interp,\\\n",
    "                    'rh':sfc_rh_interp,\\\n",
    "                    'wind_speed':sfc_wind_speed_interp,\\\n",
    "                    'wind_dir':sfc_wind_dir_interp,\\\n",
    "                    'pressure':sfc_pressure_interp,\\\n",
    "                   }\n",
    "\n",
    "        return sfc_met_present_flag,sfc_met_out_dict\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "923c44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sat(date,sat_dates_dt,sat_files):\n",
    "\n",
    "    tmpid = np.where(sat_dates_dt == date)\n",
    "    if np.size(tmpid) == 0.:\n",
    "        sat_present_flag = False\n",
    "        sat_out_dict = None\n",
    "        return sat_present_flag,sat_out_dict\n",
    "    elif np.size(tmpid) > 0.:\n",
    "        sat_present_flag = True\n",
    "        tmpid = tmpid[0][0]\n",
    "        current_sat_file = sat_files[tmpid]\n",
    "\n",
    "        ncfile = xarray.open_dataset(current_sat_file,decode_times=False)\n",
    "        sat_time_epoch = np.array(ncfile['time_epoch'].copy())\n",
    "        sat_lat = np.array(ncfile['lat'].copy())\n",
    "        sat_lon = np.array(ncfile['lon'].copy())\n",
    "        sat_visible_reflectance = np.array(ncfile['visible_reflectance'].copy())\n",
    "        sat_ir_brightness_temperature = np.array(ncfile['ir_brightness_temperature'].copy())\n",
    "        sat_effective_temperature = np.array(ncfile['effective_temperature'].copy())\n",
    "        sat_lwp = np.array(ncfile['lwp'].copy())\n",
    "        sat_iwp = np.array(ncfile['iwp'].copy())\n",
    "        sat_ice_re = np.array(ncfile['ice_re'].copy())\n",
    "        sat_ice_de = np.array(ncfile['ice_de'].copy())\n",
    "        sat_liq_re = np.array(ncfile['liq_re'].copy())\n",
    "        sat_optical_depth = np.array(ncfile['optical_depth'].copy())\n",
    "        ncfile.close()\n",
    "        sat_time_ts = sat_time_epoch.copy()\n",
    "        sat_time_dt = np.array([toDatetime(sat_time_ts[dd]) for dd in range(len(sat_time_ts))])\n",
    "        \n",
    "\n",
    "        sat_out_dict = {'time_ts':sat_time_ts,\\\n",
    "                        'time_dt':sat_time_dt,\\\n",
    "                        'lon':sat_lon,\\\n",
    "                        'lat':sat_lat,\\\n",
    "                        'visible_reflectance':sat_visible_reflectance,\\\n",
    "                        'ir_brightness_temperature':sat_ir_brightness_temperature,\\\n",
    "                        'ir_effective_temperature':sat_effective_temperature,\\\n",
    "                        'lwp':sat_lwp,\\\n",
    "                        'iwp':sat_iwp,\\\n",
    "                        'ice_re':sat_ice_re,\\\n",
    "                        'ice_de':sat_ice_de,\\\n",
    "                        'liq_re':sat_liq_re,\\\n",
    "                        'optical_depth':sat_optical_depth,\\\n",
    "                       }\n",
    "        \n",
    "\n",
    "        return sat_present_flag, sat_out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba638374-219a-49af-a300-20a232f00342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_sondes(date,sonde_dates_dt,sonde_times_dt,sonde_files,basta_times_dt,basta_height,cluster_ids,cluster_times_dt):\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    # Get current_date\n",
    "    sonde_id = np.where(sonde_dates_dt == date)[0]\n",
    "\n",
    "    # Pull out 3 potential soundings (1 on previous day, 2 on current_day)\n",
    "    \n",
    "    # The interpolation will work as follows:\n",
    "    #   If there are 3 available soundings, times before 12 UTC will be\n",
    "    #   linearly interpolated onto constant altitude levels between the\n",
    "    #   12 UTC sounding and the previous 00 UTC sounding. Times after 12\n",
    "    #   UTC are interpolated between 12 UTC and the 00 UTC sounding, which\n",
    "    #  is generally at 2315 UTC. If there is a sounding at ~2315 UTC, the\n",
    "    #  rest of the day is considerd to be consant with that sounding.\n",
    "    \n",
    "    #date = date[0]\n",
    "    \n",
    "    target_time_1 = datetime.datetime(date.year,date.month,date.day,0)\n",
    "    target_time_2 = datetime.datetime(date.year,date.month,date.day,12)\n",
    "    tmp_time_delta = datetime.timedelta(days=1)\n",
    "    target_time_3 = datetime.datetime(date.year,date.month,date.day,0) + tmp_time_delta\n",
    "    \n",
    "    sonde_times_dt = np.array(sonde_times_dt)\n",
    "    \n",
    "    tmpid = np.where((sonde_times_dt > (target_time_1 - datetime.timedelta(hours=2))) & (sonde_times_dt < (target_time_1 + datetime.timedelta(hours=2))))\n",
    "    if np.size(tmpid) > 0:\n",
    "        sonde_1_id = tmpid[0][0]\n",
    "        sonde_1_flag = True\n",
    "        sonde_1_time = sonde_times_dt[sonde_1_id]\n",
    "    else:\n",
    "        sonde_1_flag = False\n",
    "        sonde_1_id = -999.\n",
    "        sonde_1_time = -999.\n",
    "    tmpid = np.where((sonde_times_dt > (target_time_2 - datetime.timedelta(hours=2))) & (sonde_times_dt < (target_time_2 + datetime.timedelta(hours=2))))\n",
    "    if np.size(tmpid) > 0:\n",
    "        sonde_2_id = tmpid[0][0]\n",
    "        sonde_2_flag = True\n",
    "        sonde_2_time = sonde_times_dt[sonde_2_id]\n",
    "    else:\n",
    "        sonde_2_flag = False\n",
    "        sonde_2_id = -999.\n",
    "        sonde_2_time = -999.\n",
    "    tmpid = np.where((sonde_times_dt > (target_time_3 - datetime.timedelta(hours=2))) & (sonde_times_dt < (target_time_3 + datetime.timedelta(hours=2))))\n",
    "    if np.size(tmpid) > 0:\n",
    "        sonde_3_id = tmpid[0][0]\n",
    "        sonde_3_flag = True\n",
    "        sonde_3_time = sonde_times_dt[sonde_3_id]\n",
    "    else:\n",
    "        sonde_3_flag = False \n",
    "        sonde_3_id = -999.\n",
    "        sonde_3_time = -999.\n",
    "    \n",
    "    sonde_flags = [sonde_1_flag,sonde_2_flag,sonde_3_flag]\n",
    "    sonde_ids = [sonde_1_id,sonde_2_id,sonde_3_id]   \n",
    "    sonde_dict = {}\n",
    "    for jj in range(len(sonde_flags)):\n",
    "        if not sonde_flags[jj]:\n",
    "            sonde_dict[str(int(jj+1))] = None\n",
    "            continue\n",
    "        elif sonde_flags[jj]:\n",
    "            current_sonde_id = sonde_ids[jj]\n",
    "            current_sonde_file = sonde_files[current_sonde_id]\n",
    "            current_sonde_time = sonde_times_dt[current_sonde_id]\n",
    "            dumid = np.where(cluster_times_dt == current_sonde_time)\n",
    "            if np.size(dumid) > 0.:\n",
    "                current_cluster_id = cluster_ids[dumid[0][0]]\n",
    "            else:\n",
    "                current_cluster_id = None\n",
    "            \n",
    "            current_sonde_file = current_sonde_file.split('/')[-1]\n",
    "            path = '/mnt/raid/mwstanfo/micre_soundings/'\n",
    "            file_size = os.stat(path+current_sonde_file).st_size/1.e3\n",
    "            fstruct = fs(current_sonde_file,path,file_size)\n",
    "            Sondetmp = load_sonde_data(fstruct)         \n",
    "\n",
    "            max_alt = np.max(Sondetmp['alt'])\n",
    "            if max_alt < 10.:\n",
    "                print('Sonde failed to reach 10 km. Therefore omitting this sounding.')\n",
    "                sonde_dict[str(int(jj+1))] = None\n",
    "                sonde_flags[jj] = False\n",
    "                continue\n",
    "            else:\n",
    "                pass    \n",
    "    \n",
    "            sonde_dict[str(int(jj+1))] = {}\n",
    "            sonde_dict[str(int(jj+1))]['temperature'] = Sondetmp['drybulb_temp']    \n",
    "            sonde_dict[str(int(jj+1))]['rh'] = Sondetmp['RH']    \n",
    "            sonde_dict[str(int(jj+1))]['height'] = Sondetmp['alt']*1.e3\n",
    "            sonde_dict[str(int(jj+1))]['time'] = current_sonde_time\n",
    "            sonde_dict[str(int(jj+1))]['cluster_id'] = current_cluster_id\n",
    "    \n",
    "    \n",
    "    num_times = len(basta_times_dt)\n",
    "    num_heights = len(basta_height)\n",
    "    basta_times_ts = np.array([toTimestamp(basta_times_dt[dd]) for dd in range(len(basta_times_dt))])\n",
    "\n",
    "    dumid = np.where(sonde_flags)[0]\n",
    "    num_current_soundings = np.size(dumid)\n",
    "    \n",
    "    print('# of current soundings:',num_current_soundings)\n",
    "    if num_current_soundings == 0.:\n",
    "        interp_sonde_present_flag = False\n",
    "        interp_sonde_dict = None\n",
    "        return interp_sonde_present_flag, interp_sonde_dict\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    interp_sonde_present_flag = True\n",
    "    interp_sonde_dict = None\n",
    "    \n",
    "    temp_arr = []\n",
    "    rh_arr = []\n",
    "    height_arr = []\n",
    "    time_arr = []\n",
    "    cluster_id_arr = []\n",
    "    for jj in range(len(sonde_flags)):\n",
    "        if sonde_flags[jj]:\n",
    "            temp_arr.append(sonde_dict[str(int(jj+1))]['temperature'])\n",
    "            rh_arr.append(sonde_dict[str(int(jj+1))]['rh'])\n",
    "            height_arr.append(sonde_dict[str(int(jj+1))]['height'])\n",
    "            time_arr.append(sonde_dict[str(int(jj+1))]['time'])\n",
    "            cluster_id_arr.append(sonde_dict[str(int(jj+1))]['cluster_id'])\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    temp_interp_arr = []\n",
    "    rh_interp_arr = []\n",
    "    for jj in range(num_current_soundings):\n",
    "        temp_interp_arr.append(np.interp(basta_height,height_arr[jj],temp_arr[jj]))\n",
    "        rh_interp_arr.append(np.interp(basta_height,height_arr[jj],rh_arr[jj]))\n",
    "        \n",
    "    temp_interp_arr = np.array(temp_interp_arr)\n",
    "    rh_interp_arr = np.array(rh_interp_arr)\n",
    "    time_arr = np.array(time_arr)\n",
    "    time_arr_ts = np.array([toTimestamp(time_arr[dd]) for dd in range(len(time_arr))])\n",
    "    cluster_id_arr = np.array(cluster_id_arr)\n",
    "    \n",
    "    # check interpolation\n",
    "    if False:\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        ax1 = fig.add_subplot(111)\n",
    "        ax1.plot(temp_interp_arr[1],basta_height,c='b',lw=4)\n",
    "        ax1.plot(temp_arr[1],height_arr[1],c='darkorange',ls='dotted',lw=4)\n",
    "        ax1.set_ylim(0,np.max(basta_height))\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        \n",
    "    temp_interp = []\n",
    "    rh_interp = []\n",
    "    for kk in range(num_heights):\n",
    "        temp_interp.append(np.interp(basta_times_ts,time_arr_ts,temp_interp_arr[:,kk],\\\n",
    "                                                       left=temp_interp_arr[0,kk],\\\n",
    "                                                       right=temp_interp_arr[-1,kk]))        \n",
    "        rh_interp.append(np.interp(basta_times_ts,time_arr_ts,rh_interp_arr[:,kk],\\\n",
    "                                                       left=rh_interp_arr[0,kk],\\\n",
    "                                                       right=rh_interp_arr[-1,kk])) \n",
    "\n",
    "\n",
    "    temp_interp = np.array(temp_interp)\n",
    "    rh_interp = np.array(rh_interp)\n",
    "    \n",
    "    if False:\n",
    "        fig = plt.figure(figsize=(8,6))\n",
    "        ax1= fig.add_subplot(211)\n",
    "        ax2= fig.add_subplot(212)\n",
    "        \n",
    "        dumplot = ax1.contourf(basta_times_dt,basta_height,temp_interp-273.15,cmap='RdYlBu_r')\n",
    "        cbar = fig.colorbar(dumplot,ax=ax1)\n",
    "        cbar.ax.set_ylabel('Temperature [$^{\\\\circ}$]')\n",
    "        dumplot = ax2.contourf(basta_times_dt,basta_height,rh_interp,cmap='ocean')\n",
    "        cbar = fig.colorbar(dumplot,ax=ax2)\n",
    "        cbar.ax.set_ylabel('RH [%]')\n",
    "                        \n",
    "        ax1.grid(True)\n",
    "        ax2.grid(True)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "    \n",
    "\n",
    "    # calculate time to nearest sounding\n",
    "    time_to_nearest_sounding = []\n",
    "    nearest_cluster_id = []\n",
    "    # Now calculate time to nearest sounding using timedeltas\n",
    "    for ttt in range(len(basta_times_dt)):\n",
    "        single_time_ts = toTimestamp(basta_times_dt[ttt])\n",
    "\n",
    "        diff_arr = np.abs(single_time_ts - time_arr_ts)\n",
    "        dum_min = np.min(diff_arr)\n",
    "        time_to_nearest_sounding.append(dum_min)\n",
    "        \n",
    "        # Find nearest sounding\n",
    "        nearest_val,nearest_id = find_nearest(time_arr_ts,single_time_ts)\n",
    "        nearest_cluster_id.append(cluster_id_arr[nearest_id])\n",
    "    time_to_nearest_sounding = np.array(time_to_nearest_sounding)\n",
    "    nearest_cluster_id = np.array(nearest_cluster_id)\n",
    "    \n",
    "    # Boolean plot to check cluster id \"interpolation\"\n",
    "    if False:\n",
    "        fig = plt.figure(figsize=(12,4))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.plot(basta_times_dt,nearest_cluster_id)\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "                                  \n",
    "                                \n",
    "                        \n",
    "    interp_sonde_dict = {'temperature':temp_interp,'rh':rh_interp,'seconds_to_nearest_sounding':time_to_nearest_sounding,'nearest_cluster_id':nearest_cluster_id}\n",
    "    \n",
    "    return interp_sonde_present_flag,interp_sonde_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcc31fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_optics(date,optics_dates_dt,optics_files,avg_bool,basta_time_ts):\n",
    "\n",
    "    tmpid = np.where(optics_dates_dt == date)\n",
    "    if np.size(tmpid) == 0.:\n",
    "        optics_present_flag = False\n",
    "        optics_dict = None\n",
    "        return optics_present_flag,optics_dict\n",
    "    elif np.size(tmpid) > 0.:\n",
    "        optics_present_flag = True\n",
    "        tmpid = tmpid[0][0]\n",
    "        current_optics_file = optics_files[tmpid]\n",
    "        current_optics_date = optics_dates_dt[tmpid]    \n",
    "\n",
    "        ncfile = xarray.open_dataset(current_optics_file,decode_times=False)\n",
    "        base_time = ncfile['base_time'].values\n",
    "        time_offset = ncfile['time_offset'].values\n",
    "        qc_time = ncfile['qc_time'].values\n",
    "        tau_inst = ncfile['optical_depth_instantaneous'].values\n",
    "        qc_tau_inst = ncfile['qc_optical_depth_instantaneous'].values\n",
    "        reff_inst = ncfile['effective_radius_instantaneous'].values\n",
    "        qc_reff_inst = ncfile['qc_effective_radius_instantaneous'].values\n",
    "        tau_avg = ncfile['optical_depth_instantaneous'].values\n",
    "        qc_tau_avg = ncfile['qc_optical_depth_average'].values\n",
    "        reff_avg = ncfile['effective_radius_average'].values\n",
    "        qc_reff_avg = ncfile['qc_effective_radius_average'].values\n",
    "        trans1 = ncfile['total_transmittance_filter1'].values\n",
    "        trans2 = ncfile['total_transmittance_filter2'].values\n",
    "        trans3 = ncfile['total_transmittance_filter3'].values\n",
    "        trans4 = ncfile['total_transmittance_filter4'].values\n",
    "        trans5 = ncfile['total_transmittance_filter5'].values\n",
    "        source_lwp = ncfile['source_lwp'].values\n",
    "        lwp = ncfile['lwp'].values\n",
    "        qc_lwp = ncfile['qc_lwp'].values\n",
    "        pwv = ncfile['pwv'].values\n",
    "        qc_pwv = ncfile['qc_pwv'].values\n",
    "        cf = ncfile['cloudfraction'].values\n",
    "        qc_cf = ncfile['qc_cloudfraction'].values\n",
    "        ncfile.close()\n",
    "        time_ts = base_time + time_offset\n",
    "        time_dt = np.array([toDatetime(time_ts[dd]) for dd in range(len(time_ts))])\n",
    "        \n",
    "        tau_inst_interp = np.interp(basta_time_ts,time_ts,tau_inst)\n",
    "        tau_avg_interp = np.interp(basta_time_ts,time_ts,tau_avg)\n",
    "        trans1_interp = np.interp(basta_time_ts,time_ts,trans1)\n",
    "        trans2_interp = np.interp(basta_time_ts,time_ts,trans2)\n",
    "        trans3_interp = np.interp(basta_time_ts,time_ts,trans3)\n",
    "        trans4_interp = np.interp(basta_time_ts,time_ts,trans4)\n",
    "        trans5_interp = np.interp(basta_time_ts,time_ts,trans5)\n",
    "        ref_inst_interp = np.interp(basta_time_ts,time_ts,reff_inst)\n",
    "        reff_avg_interp = np.interp(basta_time_ts,time_ts,reff_avg)\n",
    "        lwp_interp = np.interp(basta_time_ts,time_ts,lwp)\n",
    "        cf_interp = np.interp(basta_time_ts,time_ts,cf)\n",
    "        \n",
    "        optics_dict = {'tau_inst':tau_inst_interp,\\\n",
    "                       'tau_avg':tau_avg_interp,\\\n",
    "                       'reff_inst':tau_avg_interp,\\\n",
    "                       'reff_avg':tau_avg_interp,\\\n",
    "                       'trans1':trans1_interp,\\\n",
    "                       'trans2':trans2_interp,\\\n",
    "                       'trans3':trans3_interp,\\\n",
    "                       'trans4':trans4_interp,\\\n",
    "                       'trans5':trans5_interp,\\\n",
    "                       'cloud_fraction':cf_interp,\\\n",
    "                       'lwp':lwp_interp,\\\n",
    "                       'native_tau_inst':tau_inst,\\\n",
    "                       'native_tau_avg':tau_avg,\\\n",
    "                       'native_trans1':trans1,\\\n",
    "                       'native_trans2':trans2,\\\n",
    "                       'native_trans3':trans3,\\\n",
    "                       'native_trans4':trans4,\\\n",
    "                       'native_trans5':trans5,\\\n",
    "                       'native_reff_inst':reff_inst,\\\n",
    "                       'native_reff_avg':reff_avg,\\\n",
    "                       'native_lwp':lwp,\\\n",
    "                       'native_lwp':pwv,\\\n",
    "                       'native_source_lwp':source_lwp,\\\n",
    "                       'native_time_dt':time_dt,\\\n",
    "                       'native_time_ts':time_ts,\\\n",
    "                       'native_cloud_fraction':cf,\\\n",
    "                      }\n",
    "        \n",
    "        return optics_present_flag,optics_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a1c8bc97-bc90-40d7-b1fc-e2ffeab0d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================\n",
    "# Function to process basta data\n",
    "#\n",
    "# Inputs: date -- from basta_dates_dt\n",
    "#         avg_bool -- boolean to perform averaging\n",
    "#==================================================\n",
    "def process_basta(date,basta_dates_dt,basta_files,avg_bool):\n",
    "    # grab date\n",
    "    dumid = np.where(basta_dates_dt == date)\n",
    "    dumid = dumid[0][0]\n",
    "    \n",
    "    # read in file\n",
    "    ncfile = xarray.open_dataset(basta_files[dumid],decode_times=False)\n",
    "    basta_time_dims = ncfile.dims['time'] # will be variable according to up-time\n",
    "    basta_height_dims = ncfile.dims['height'] # should always be 480\n",
    "    basta_ref = np.array(ncfile['reflectivity'].copy())\n",
    "    basta_vel = np.array(ncfile['velocity'].copy())\n",
    "    basta_flag = np.array(ncfile['flag'].copy())\n",
    "    basta_flag_coupling = np.array(ncfile['flag_coupling'].copy()) # 0: no coupling (good); 1: coupling (bad)\n",
    "    basta_noise_level = np.array(ncfile['noise_level'].copy()) # 0: good data; 1-9: bad data; -1: no data\n",
    "    basta_time_sec_since_00Z = np.array(ncfile['time'].copy())\n",
    "    basta_height = np.array(ncfile['height'].copy()) # 25-m resolution beginning at 12.5 m (mid-bin)\n",
    "    ### ends at 11987.5 m, so 12 km\n",
    "    ncfile.close()\n",
    "    \n",
    "    tmp_basta_time_ts = toTimestamp(datetime.datetime(date.year,\\\n",
    "                                       date.month,\\\n",
    "                                       date.day))\n",
    "    tmp_basta_time_ts = tmp_basta_time_ts + basta_time_sec_since_00Z\n",
    "    basta_time_dt = [toDatetime(tmp_basta_time_ts[dd]) for dd in range(len(tmp_basta_time_ts))]\n",
    "    basta_time_dt = np.array(basta_time_dt) # holds the BASTA time array for the current file.\n",
    "    \n",
    "    # Need to ensure BASTA times are stricty increasing.\n",
    "    # This is a rare occurrence--only 2 days. Informed\n",
    "    # Alain Protat, but just omitting these cases.\n",
    "    diff = np.diff(tmp_basta_time_ts)\n",
    "\n",
    "    if np.min(diff) < 0.:\n",
    "        #raise RuntimeError('BASTA times are not strictly increasing.')\n",
    "        basta_present_flag = False\n",
    "        basta_out_dict = None\n",
    "        return basta_present_flag, basta_out_dict\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    basta_present_flag = True\n",
    "    \n",
    "    #------------------------------------------------------\n",
    "    # For some of the files, the date after the current one\n",
    "    # holds the last hour of the day. In these cases, will\n",
    "    # need to pull in the following day.\n",
    "    #------------------------------------------------------\n",
    "    \n",
    "    # Check first time of following file and grab times and associated\n",
    "    # variables where the DAY matches that of the current BASTA day.\n",
    "    # Grab next file\n",
    "    if (date != datetime.datetime(2016,4,2)) and (date != datetime.datetime(2017,3,17)):\n",
    "\n",
    "        ncfile = xarray.open_dataset(basta_files[dumid+1],decode_times=False)\n",
    "\n",
    "        after_basta_time_sec_since_00Z = np.array(ncfile['time'].copy())\n",
    "        ncfile.close()\n",
    "        tmp_basta_time_ts = toTimestamp(datetime.datetime(basta_dates_dt[dumid+1].year,\\\n",
    "                                           basta_dates_dt[dumid+1].month,\\\n",
    "                                           basta_dates_dt[dumid+1].day))\n",
    "\n",
    "        tmp_basta_time_ts = tmp_basta_time_ts + after_basta_time_sec_since_00Z\n",
    "        after_basta_time_dt = [toDatetime(tmp_basta_time_ts[dd]) for dd in range(len(tmp_basta_time_ts))]\n",
    "        after_basta_date_dt = [datetime.datetime(after_basta_time_dt[dd].year,\\\n",
    "                                                after_basta_time_dt[dd].month,\\\n",
    "                                                after_basta_time_dt[dd].day) for dd in range(len(after_basta_time_dt))]\n",
    "        after_basta_time_dt = np.array(after_basta_time_dt) # holds the BASTA time array for the after file.\n",
    "        after_basta_date_dt = np.array(after_basta_date_dt) # holds the BASTA date array for the after file.\n",
    "        \n",
    "        # check to see if any of the dates in the after file equal the date on the current file\n",
    "        #date = date[0]\n",
    "        tmpid = np.where(after_basta_date_dt == date)\n",
    "        if np.size(tmpid) > 0.:\n",
    "            # now open back up after file and add indices in after file with\n",
    "            # same date as current file to the current BASTA arrays\n",
    "            ncfile = xarray.open_dataset(basta_files[dumid+1],decode_times=False)\n",
    "            after_basta_ref = np.array(ncfile['reflectivity'].copy())\n",
    "            after_basta_vel = np.array(ncfile['velocity'].copy())\n",
    "            after_basta_flag = np.array(ncfile['flag'].copy())\n",
    "            after_basta_flag_coupling = np.array(ncfile['flag_coupling'].copy()) # 0: no coupling (good); 1: coupling (bad)\n",
    "            after_basta_noise_level = np.array(ncfile['noise_level'].copy()) # 0: good data; 1-9: bad data; -1: no data\n",
    "            ncfile.close()  \n",
    "            \n",
    "            # now concatenate arrays\n",
    "            basta_time_dt = np.concatenate((basta_time_dt,after_basta_time_dt[tmpid]))\n",
    "            basta_ref = np.concatenate((basta_ref,np.squeeze(after_basta_ref[:,tmpid])),axis=1)\n",
    "            basta_vel = np.concatenate((basta_vel,np.squeeze(after_basta_vel[:,tmpid])),axis=1)\n",
    "            basta_flag = np.concatenate((basta_flag,np.squeeze(after_basta_flag[:,tmpid])),axis=1)\n",
    "            basta_flag_coupling = np.concatenate((basta_flag_coupling,after_basta_flag_coupling[tmpid]),axis=0)\n",
    "            basta_noise_level = np.concatenate((basta_noise_level,after_basta_noise_level[tmpid]),axis=0)\n",
    "            \n",
    "\n",
    "                \n",
    "    # Because the current date BASTA file sometimes start at 23Z on the day prior, also need to\n",
    "    # limit the current file to encompass only times on the current date (i.e., need to limit\n",
    "    # the current date variables, filtering out those from 23Z-00Z on the previous date)\n",
    "    basta_date_dt = np.array([datetime.datetime(basta_time_dt[dd].year,\\\n",
    "                                                basta_time_dt[dd].month,\\\n",
    "                                                basta_time_dt[dd].day) for dd in range(len(basta_time_dt))])\n",
    "    \n",
    "    # Check to see if any of the dates in the current file equal the before date\n",
    "    if (date != datetime.datetime(2016,4,2)) and (date != datetime.datetime(2017,3,17)):\n",
    "        tmpid = np.where(basta_date_dt != date)\n",
    "        if np.size(tmpid) > 0.:\n",
    "            # limit arrays\n",
    "            tmpid = np.where(basta_date_dt == date)\n",
    "            basta_time_dt = basta_time_dt[tmpid]\n",
    "            basta_flag_coupling = basta_flag_coupling[tmpid]\n",
    "            basta_flag = basta_flag[:,tmpid]\n",
    "            basta_ref = basta_ref[:,tmpid]\n",
    "            basta_vel = basta_vel[:,tmpid]\n",
    "            basta_noise_level = basta_noise_level[tmpid]\n",
    "\n",
    "    \n",
    "    basta_ref = np.squeeze(basta_ref)\n",
    "    basta_vel = np.squeeze(basta_vel)\n",
    "    basta_flag = np.squeeze(basta_flag)\n",
    "    basta_flag_coupling = np.squeeze(basta_flag_coupling)\n",
    "    basta_noise_level = np.squeeze(basta_noise_level)         \n",
    "        \n",
    "        \n",
    "    #-----------------------------------------\n",
    "    #-----------------------------------------\n",
    "    #-----------------------------------------\n",
    "    # Create a series of flags that will indicate\n",
    "    # whether or not a cloud is present\n",
    "    # \n",
    "    # basta_flag == 1 means radar is working\n",
    "    # properly but there is no data. These values\n",
    "    # are flagged as -999. When basta_flag > 0.\n",
    "    # or when basta_flag_coupling > 0., we will\n",
    "    # assign these values as NaNs. For values\n",
    "    # below the theoretical minimum detectable signal,\n",
    "    # we will assign these as -999. as well. Values\n",
    "    # up to 137.5 m will be flagged as NaNs, indicating\n",
    "    # bad data.\n",
    "    #-----------------------------------------\n",
    "    #-----------------------------------------\n",
    "    #-----------------------------------------\n",
    "    \n",
    "    bad_radar_data_flag = np.zeros(len(basta_time_dt))\n",
    "    min_basta_loc = 6\n",
    "    # create array that is the minimum detectable signal as a function of altitude\n",
    "    Z_min_1km = -36.\n",
    "    ref_range = 1000.\n",
    "    Z_min = Z_min_1km + 20.*np.log10(basta_height) - 20.*np.log10(ref_range)\n",
    "    Z_min[0] = -999.    \n",
    "\n",
    "    # NaN out values up to 137.5 m due to surface clutter\n",
    "    basta_ref[0:min_basta_loc,:] = np.nan\n",
    "    basta_vel[0:min_basta_loc,:] = np.nan\n",
    "    # We will also assign all basta_flag values up to 137.5 m\n",
    "    # as -1. Currently basta_flag == -1 only up to 87.5 m, so we\n",
    "    # want to adjust this.\n",
    "    basta_flag[0:min_basta_loc,:] = -1\n",
    "            \n",
    "        \n",
    "    # Set values below the minimum detectable signal to -999.\n",
    "    for ttt in range(len(basta_time_dt)):\n",
    "        dumid = np.where(basta_ref[:,ttt] < Z_min)\n",
    "        if np.size(dumid) > 0.:\n",
    "            basta_ref[dumid,ttt] = -999.\n",
    "            basta_vel[dumid,ttt] = -999.\n",
    "            basta_flag[dumid,ttt] = -1\n",
    "    \n",
    "    dumid = np.where(basta_flag_coupling == 1.)\n",
    "    if np.size(dumid) > 0.:\n",
    "        basta_ref[:,dumid] = np.nan\n",
    "        basta_vel[:,dumid] = np.nan\n",
    "        bad_radar_data_flag[dumid] = 1\n",
    "        \n",
    "    for ttt in range(len(basta_time_dt)):\n",
    "        single_time_basta_flag = basta_flag[:,ttt]\n",
    "        dumid = np.where(single_time_basta_flag > 0.)\n",
    "        if np.size(dumid) > 0.:\n",
    "            basta_ref[dumid,ttt] = np.nan\n",
    "            basta_vel[dumid,ttt] = np.nan\n",
    "        if np.all(single_time_basta_flag > 0.):\n",
    "            bad_radar_data_flag[ttt] = 1           \n",
    "    \n",
    "    basta_time_ts = np.array([toTimestamp(basta_time_dt[dd]) for dd in range(len(basta_time_dt))])\n",
    "    basta_out_dict = {'time_dt':basta_time_dt,\\\n",
    "                      'time_ts':basta_time_ts,\\\n",
    "                      'ref':basta_ref,\\\n",
    "                      'vel':basta_vel,\\\n",
    "                      'height':basta_height,\\\n",
    "                      'bad_radar_data_flag':bad_radar_data_flag,\\\n",
    "                      'flag':basta_flag,\\\n",
    "                     }\n",
    "    \n",
    "    return basta_present_flag,basta_out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cdbc2768-1094-4317-9cb8-6535fa3ea487",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_arm_ceil(date,arm_ceil_dates_dt,arm_ceil_files,avg_bool,basta_height,basta_time_ts,interp_backscatter=False):\n",
    "    tmpid = np.where(arm_ceil_dates_dt == date)\n",
    "    if np.size(tmpid) == 0.:\n",
    "        arm_ceil_present_flag = False\n",
    "        arm_ceil_out_dict = None\n",
    "        return arm_ceil_present_flag, arm_ceil_out_dict\n",
    "    elif np.size(tmpid) > 0.:        \n",
    "        arm_ceil_present_flag = True\n",
    "        tmpid = tmpid[0][0]\n",
    "        current_ceil_file = arm_ceil_files[tmpid]\n",
    "        ncfile = xarray.open_dataset(current_ceil_file,decode_times=False)\n",
    "        ceil_dims = ncfile.dims\n",
    "        ceil_base_time = np.array(ncfile['base_time'].copy())        \n",
    "        ceil_num_times = ceil_dims['time']\n",
    "        ceil_time_offset = np.array(ncfile['time_offset'].copy())\n",
    "        ceil_cbh_1 = np.array(ncfile['first_cbh'].copy())\n",
    "        ceil_qc_cbh_1 = np.array(ncfile['qc_first_cbh'].copy())\n",
    "        ceil_cbh_2 = np.array(ncfile['second_cbh'].copy())\n",
    "        ceil_qc_cbh_2 = np.array(ncfile['qc_second_cbh'].copy())\n",
    "        ceil_cbh_3 = np.array(ncfile['third_cbh'].copy())\n",
    "        ceil_qc_cbh_3 = np.array(ncfile['qc_third_cbh'].copy())\n",
    "        ceil_backscatter = np.array(ncfile['backscatter'].copy())\n",
    "        ceil_sum_backscatter = np.array(ncfile['sum_backscatter'].copy())\n",
    "        ceil_qc_sum_backscatter = np.array(ncfile['qc_sum_backscatter'].copy())\n",
    "        ceil_detection_status = np.array(ncfile['detection_status'].copy())\n",
    "        ceil_time_ts = [int(ceil_base_time + ceil_time_offset[dd]) for dd in range(ceil_num_times)]\n",
    "        ceil_time_dt = [toDatetime(ceil_time_ts[dd]) for dd in range(ceil_num_times)]    \n",
    "        ceil_range_bounds = np.array(ncfile['range_bounds'].copy())\n",
    "        ceil_range = np.array(ncfile['range'].copy())\n",
    "        ncfile.close()\n",
    "\n",
    "        # qc backscatter\n",
    "        dumxid = np.where(ceil_qc_sum_backscatter > 0)\n",
    "        if np.size(dumxid) > 0.:\n",
    "            #dumxid = np.squeeze(dumxid)\n",
    "            ceil_backscatter[dumxid,:] = np.nan \n",
    "            ceil_sum_backscatter[dumxid,:] = np.nan \n",
    "        ceil_backscatter = ceil_backscatter*1.e-3/10000.  \n",
    "        \n",
    "        ceil_backscatter_log = ceil_backscatter.copy()\n",
    "        dumid = np.where((~np.isnan(ceil_backscatter)) & (ceil_backscatter > 0.) )\n",
    "        ceil_backscatter_log[dumid] = np.log10(ceil_backscatter[dumid])\n",
    "        dumid = np.where(ceil_backscatter == 0.)\n",
    "        ceil_backscatter_log[dumid] = np.nan\n",
    "        dumid = np.where(ceil_backscatter < 0.)\n",
    "        ceil_backscatter_log[dumid] = np.nan\n",
    "        #ceil_backscatter_log[np.isnan(ceil_backscatter_log)] = 0. \n",
    "        ceil_backscatter = ceil_backscatter_log\n",
    "\n",
    "        #------------------------------------------\n",
    "        # Interpolate ceilometer to radar time grid\n",
    "        # using nearest neighbor interpolation. \n",
    "        # This method requires that the nearest\n",
    "        # neighbor be within 15 seconds of the\n",
    "        # radar time grid element.\n",
    "        #------------------------------------------\n",
    "        ceil_time_ts = np.array([toTimestamp(ceil_time_dt[dd]) for dd in range(len(ceil_time_dt))])\n",
    "        basta_bin_edges = np.arange(0,np.max(basta_height)+12.5+25.,25.)\n",
    "        basta_height = np.around(basta_height,2)\n",
    "        basta_time_dt = np.array([toDatetime(basta_time_ts[dd]) for dd in range(len(basta_time_ts))])\n",
    "        ceil_cbh_1_interp = []\n",
    "        ceil_cbh_2_interp = []\n",
    "        ceil_cbh_3_interp = []\n",
    "        ceil_detection_status_interp = []\n",
    "        ceil_cbh_bin_relative_interp = []\n",
    "        for ttt in range(len(basta_time_ts)):\n",
    "            # if here, then good radar data exists\n",
    "            # Now find the nearest in time ceilometer time step to the radar time step\n",
    "            # If the ceilometer is more than 15 seconds away from the the radar time step,\n",
    "            # then we will flag it as missing data (NaN)\n",
    "            nearest_val,nearest_id = find_nearest(ceil_time_ts,basta_time_ts[ttt])\n",
    "            time_diff = np.abs(nearest_val - basta_time_ts[ttt])\n",
    "            target_time_diff = 16\n",
    "            if time_diff <= target_time_diff:\n",
    "                nearest_ceil_cbh_1 = ceil_cbh_1[nearest_id]\n",
    "                nearest_ceil_cbh_2 = ceil_cbh_2[nearest_id]\n",
    "                nearest_ceil_cbh_3 = ceil_cbh_3[nearest_id]\n",
    "                nearest_ceil_detection_status = ceil_detection_status[nearest_id]\n",
    "                ceil_detection_status_interp.append(nearest_ceil_detection_status)\n",
    "\n",
    "                if np.isnan(nearest_ceil_detection_status):\n",
    "                    #ceil_detection_status_interp.append(np.nan)\n",
    "                    ceil_cbh_1_interp.append(np.nan)\n",
    "                    ceil_cbh_2_interp.append(np.nan)\n",
    "                    ceil_cbh_3_interp.append(np.nan)\n",
    "                    ceil_cbh_bin_relative_interp.append(np.nan)\n",
    "                    continue\n",
    "                    \n",
    "                elif (nearest_ceil_detection_status == 5.) or (nearest_ceil_detection_status == 0.):\n",
    "                    ceil_cbh_1_interp.append(-999.)\n",
    "                    ceil_cbh_2_interp.append(-999.)\n",
    "                    ceil_cbh_3_interp.append(-999.)\n",
    "                    ceil_cbh_bin_relative_interp.append(-999.)\n",
    "                    continue\n",
    "                    \n",
    "                elif (nearest_ceil_detection_status == 4.):\n",
    "                    ceil_cbh_1_interp.append(np.nan)\n",
    "                    ceil_cbh_2_interp.append(np.nan)\n",
    "                    ceil_cbh_3_interp.append(np.nan)\n",
    "                    ceil_cbh_bin_relative_interp.append(np.nan)\n",
    "                    continue                    \n",
    "                    \n",
    "                elif (nearest_ceil_detection_status == 1.) or (nearest_ceil_detection_status == 2.) or (nearest_ceil_detection_status == 3.):\n",
    "                    pass\n",
    "                else:\n",
    "                    raise RuntimeError('Something went wrong')\n",
    "\n",
    "                #if np.isnan(nearest_ceil_cbh_1):\n",
    "                #    ceil_cbh_1_interp.append(np.nan)\n",
    "                #    ceil_cbh_2_interp.append(np.nan)\n",
    "                #    ceil_cbh_3_interp.append(np.nan)\n",
    "                #    ceil_cbh_bin_relative_interp.append(np.nan)\n",
    "                #    continue\n",
    "                \n",
    "                if np.isnan(nearest_ceil_cbh_1):\n",
    "                    print(nearest_ceil_cbh_1)\n",
    "                    print(nearest_ceil_detection_status)\n",
    "                    raise RuntimeError('Something went wrong')\n",
    "                    \n",
    "                nearest_val,nearest_id = find_nearest(basta_bin_edges,nearest_ceil_cbh_1)\n",
    "                if nearest_ceil_cbh_1 == nearest_val:\n",
    "                    bin_edges = basta_bin_edges[nearest_id-1:nearest_id+1]\n",
    "                    midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                    ceil_cbh_1_interp.append(midbin)\n",
    "                    ceil_cbh_bin_relative_interp.append(1.)\n",
    "                elif nearest_ceil_cbh_1 < nearest_val:\n",
    "                    bin_edges = basta_bin_edges[nearest_id-1:nearest_id+1]\n",
    "                    midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                    ceil_cbh_1_interp.append(midbin)\n",
    "                    ceil_cbh_bin_relative_interp.append(0.)\n",
    "                elif nearest_ceil_cbh_1 > nearest_val:\n",
    "                    bin_edges = basta_bin_edges[nearest_id:nearest_id+2]\n",
    "                    midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                    ceil_cbh_1_interp.append(midbin)\n",
    "                    ceil_cbh_bin_relative_interp.append(2.)\n",
    "                    \n",
    "                if np.isnan(nearest_ceil_cbh_2):\n",
    "                    ceil_cbh_2_interp.append(np.nan)\n",
    "                else:    \n",
    "                    nearest_val,nearest_id = find_nearest(basta_bin_edges,nearest_ceil_cbh_2)\n",
    "                    if nearest_ceil_cbh_2 == nearest_val:\n",
    "                        bin_edges = basta_bin_edges[nearest_id-1:nearest_id+1]\n",
    "                        midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                        ceil_cbh_2_interp.append(midbin)\n",
    "                    elif nearest_ceil_cbh_2 < nearest_val:\n",
    "                        bin_edges = basta_bin_edges[nearest_id-1:nearest_id+1]\n",
    "                        midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                        ceil_cbh_2_interp.append(midbin)\n",
    "                    elif nearest_ceil_cbh_2 > nearest_val:\n",
    "                        bin_edges = basta_bin_edges[nearest_id:nearest_id+2]\n",
    "                        midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                        ceil_cbh_2_interp.append(midbin)\n",
    "\n",
    "                if np.isnan(nearest_ceil_cbh_3):\n",
    "                    ceil_cbh_3_interp.append(np.nan)\n",
    "                else:                      \n",
    "                    nearest_val,nearest_id = find_nearest(basta_bin_edges,nearest_ceil_cbh_3)\n",
    "                    if nearest_ceil_cbh_3 == nearest_val:\n",
    "                        bin_edges = basta_bin_edges[nearest_id-1:nearest_id+1]\n",
    "                        midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                        ceil_cbh_3_interp.append(midbin)\n",
    "                    elif nearest_ceil_cbh_3 < nearest_val:\n",
    "                        bin_edges = basta_bin_edges[nearest_id-1:nearest_id+1]\n",
    "                        midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                        ceil_cbh_3_interp.append(midbin)\n",
    "                    elif nearest_ceil_cbh_3 > nearest_val:\n",
    "                        bin_edges = basta_bin_edges[nearest_id:nearest_id+2]\n",
    "                        midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                        ceil_cbh_3_interp.append(midbin)                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "            else:\n",
    "                #print('here')\n",
    "                #print(time_diff,basta_time_dt[ttt],ceil_time_dt[nearest_id])\n",
    "                ceil_cbh_1_interp.append(np.nan)\n",
    "                ceil_cbh_2_interp.append(np.nan)\n",
    "                ceil_cbh_3_interp.append(np.nan)\n",
    "                ceil_detection_status_interp.append(np.nan)\n",
    "                ceil_cbh_bin_relative_interp.append(np.nan)\n",
    "\n",
    "        ceil_cbh_1_interp = np.array(ceil_cbh_1_interp)\n",
    "        ceil_cbh_2_interp = np.array(ceil_cbh_2_interp)\n",
    "        ceil_cbh_3_interp = np.array(ceil_cbh_3_interp)\n",
    "        ceil_detection_status_interp = np.array(ceil_detection_status_interp)    \n",
    "        ceil_cbh_bin_relative_interp = np.array(ceil_cbh_bin_relative_interp)  \n",
    "        \n",
    "        \n",
    "        #============================================\n",
    "        # Plot to explore backscatter interpolation\n",
    "        #============================================\n",
    "\n",
    "        #if True:\n",
    "        if False:\n",
    "\n",
    "            fig = plt.figure(figsize=(24,14))\n",
    "            Fontsize=14\n",
    "            dfmt = mdates.DateFormatter('%H:%M')\n",
    "            ax_native = fig.add_subplot(211)\n",
    "            ax_interp_nn = fig.add_subplot(212)\n",
    "            #ax_interp_cubic1 = fig.add_subplot(223)\n",
    "            #ax_interp_linear = fig.add_subplot(224)\n",
    "            \n",
    "            basta_time_dt = np.array([toDatetime(basta_time_ts[dd]) for dd in range(len(basta_time_ts))])             \n",
    "            start_time = datetime.datetime(basta_time_dt[0].year,basta_time_dt[0].month,basta_time_dt[0].day,0,0)\n",
    "            #end_time = start_time + datetime.timedelta(days=1)           \n",
    "            end_time = start_time + datetime.timedelta(hours=24)           \n",
    "            \n",
    "            \n",
    "            #axlist = [ax_native,ax_interp_nn,ax_interp_cubic1,ax_interp_linear]\n",
    "            axlist = [ax_native,ax_interp_nn]\n",
    "            for ax in axlist:\n",
    "                ax.tick_params(labelsize=Fontsize)\n",
    "                ax.set_ylabel('Height [km]',fontsize=Fontsize)\n",
    "                ax.set_xlabel('UTC Time [HH:MM]',fontsize=Fontsize)\n",
    "                ax.xaxis.set_major_formatter(dfmt)\n",
    "                ax.grid(which='both',c='dimgrey',ls='dotted',lw=1)\n",
    "                ax.set_xlim(start_time,end_time)\n",
    "                ax.set_ylim(0,1)\n",
    "                \n",
    "            cmap = matplotlib.cm.get_cmap(\"jet\").copy()\n",
    "            cmap.set_under('navy')\n",
    "            cmap.set_bad('grey')\n",
    "\n",
    "            # Native\n",
    "            height_bins = ceil_range_bounds[:,0]\n",
    "            dumbin = np.array([height_bins[-1]+30])\n",
    "            height_bins = np.concatenate((height_bins,dumbin))\n",
    "\n",
    "            native_plot = ax_native.pcolormesh(ceil_time_dt,\\\n",
    "                                                             height_bins*1.e-3,\\\n",
    "                                                             ceil_backscatter[1:,:].T,\\\n",
    "                                                             cmap=cmap,\n",
    "                                                             vmin=-8,vmax=-3)\n",
    "            # Colorbar\n",
    "            dum_ticks = [-8,-7,-6,-5,-4,-3]\n",
    "            native_cbar = fig.colorbar(native_plot,ticks=dum_ticks,pad=0.01,ax=ax_native)\n",
    "            dumstr = '$log_{10}$($\\\\beta_{att}$)'\n",
    "            native_cbar.ax.set_ylabel(dumstr,fontsize=Fontsize)\n",
    "            native_cbar.ax.tick_params(labelsize=Fontsize)  \n",
    "            \n",
    "            ax_native.set_title('Native $\\\\beta_{att}$ \\n 30-m x 16-sec resolution',fontsize=Fontsize*1.5,color='dimgrey')\n",
    "            ax_native.scatter(ceil_time_dt,ceil_cbh_1*1.e-3,s=2,c='black')\n",
    "\n",
    "            \n",
    "            # Fill in obscured time periods with transparent red\n",
    "            id4 = np.where(ceil_detection_status == 4.)\n",
    "            detection_mask = np.zeros(np.shape(ceil_detection_status))\n",
    "            if np.size(id4) > 1.:\n",
    "                id4 = np.squeeze(id4)\n",
    "                detection_mask[id4] = 1\n",
    "                detection_Objects,num_detection_objects = ndimage.label(detection_mask)\n",
    "                for kk in range(num_detection_objects):\n",
    "                    dumid = np.where(detection_Objects == kk+1)[0]\n",
    "                    first_id = dumid[0]\n",
    "                    last_id = dumid[-1]\n",
    "                    ax_native.axvspan(ceil_time_dt[first_id],\\\n",
    "                                ceil_time_dt[last_id],color='red',alpha=0.5)               \n",
    "            \n",
    "            \n",
    "            \n",
    "            ceil_time_dt = np.array(ceil_time_dt)\n",
    "            ceil_time_dt_orig = ceil_time_dt.copy()\n",
    "            ceil_time_ts_orig = ceil_time_ts.copy()\n",
    "            \n",
    "            dumid = np.where( (ceil_time_ts >= basta_time_ts[0]) & (ceil_time_ts <= basta_time_ts[-1]))\n",
    "            if np.size(dumid) > 0.:\n",
    "                dumid = np.squeeze(dumid)\n",
    "                ceil_time_ts = ceil_time_ts[dumid]\n",
    "                ceil_time_dt = ceil_time_dt[dumid]\n",
    "                ceil_backscatter = ceil_backscatter[dumid,:]\n",
    "\n",
    "            # Interpolated in log10 space (nearest neighbor)            \n",
    "            x=ceil_time_ts = np.array([toTimestamp(ceil_time_dt[dd]) for dd in range(len(ceil_time_dt))])\n",
    "            y=ceil_range\n",
    "            #mask invalid values\n",
    "            z = ceil_backscatter.copy()\n",
    "            z = z.T\n",
    "            xx, yy = np.meshgrid(x, y)\n",
    "            basta_height_lim = basta_height[basta_height < np.max(ceil_range)]\n",
    "            newy = basta_height_lim\n",
    "            newx = basta_time_ts\n",
    "            \n",
    "            newX,newY = np.meshgrid(newx,newy)\n",
    "            ceil_backscatter_interp_nn = griddata((xx.ravel(), yy.ravel()), z.ravel(),(newX, newY),method='nearest',fill_value=np.nan)\n",
    "        \n",
    "            dumid = np.where( (basta_time_ts < ceil_time_ts_orig[0]) | (basta_time_ts > ceil_time_ts_orig[-1]) )\n",
    "            if np.size(dumid) > 0.:\n",
    "                dumid = np.squeeze(dumid)\n",
    "                ceil_cbh_1_interp[dumid] = np.nan\n",
    "                ceil_cbh_2_interp[dumid] = np.nan\n",
    "                ceil_cbh_3_interp[dumid] = np.nan\n",
    "                ceil_backscatter_interp_nn[:,dumid] = np.nan\n",
    "                ceil_detection_status_interp[dumid] = np.nan\n",
    "                ceil_cbh_bin_relative_interp[dumid] = np.nan   \n",
    "\n",
    "                 \n",
    "            # Need to deal with missing times in between the start and end time of the radar\n",
    "            # Let's loop through basta times and if the time is not within 12 seconds of a\n",
    "            # radar profile, we'll nan it out\n",
    "\n",
    "            for dum_tt in range(len(basta_time_dt)):\n",
    "                dum_diff = np.abs(basta_time_ts[dum_tt] - ceil_time_ts)\n",
    "                if np.min(dum_diff) > 12.:\n",
    "                    ceil_backscatter_interp_nn[:,dum_tt] = np.nan               \n",
    "                \n",
    "                \n",
    "                \n",
    "            basta_height_bins = np.arange(0,np.max(basta_height_lim),25)\n",
    "            dumbins = np.array([np.max(basta_height_lim)+12.5])\n",
    "            basta_height_bins = np.concatenate((basta_height_bins,dumbins))\n",
    "            interp_nn_plot = ax_interp_nn.pcolormesh(basta_time_dt,\\\n",
    "                                                             basta_height_bins*1.e-3,\\\n",
    "                                                             ceil_backscatter_interp_nn[:,1:],\\\n",
    "                                                             cmap=cmap,\n",
    "                                                             vmin=-8,vmax=-3)      \n",
    "            \n",
    "\n",
    "            # Colorbar\n",
    "            dum_ticks = [-8,-7,-6,-5,-4,-3]\n",
    "            interp_nn_cbar = fig.colorbar(interp_nn_plot,ticks=dum_ticks,pad=0.01,ax=ax_interp_nn)\n",
    "            dumstr = '$log_{10}$($\\\\beta_{att}$)'\n",
    "            interp_nn_cbar.ax.set_ylabel(dumstr,fontsize=Fontsize)\n",
    "            interp_nn_cbar.ax.tick_params(labelsize=Fontsize)  \n",
    "            \n",
    "            ax_interp_nn.set_title('Nearest Neighbor Interpolation $\\\\beta_{att}$ \\n 25-m x 12-sec resolution',fontsize=Fontsize*1.5,color='dimgrey')            \n",
    "            # Interpolated CBH\n",
    "            ax_interp_nn.scatter(basta_time_dt,ceil_cbh_1_interp*1.e-3,s=2,c='black')\n",
    "\n",
    "            \n",
    "            # Fill in obscured time periods with transparent red\n",
    "            id4 = np.where(ceil_detection_status_interp == 4.)\n",
    "            detection_mask = np.zeros(np.shape(ceil_detection_status_interp))\n",
    "            if np.size(id4) > 1.:\n",
    "                id4 = np.squeeze(id4)\n",
    "                detection_mask[id4] = 1\n",
    "                detection_Objects,num_detection_objects = ndimage.label(detection_mask)\n",
    "                for kk in range(num_detection_objects):\n",
    "                    dumid = np.where(detection_Objects == kk+1)[0]\n",
    "                    first_id = dumid[0]\n",
    "                    last_id = dumid[-1]\n",
    "                    ax_interp_nn.axvspan(basta_time_dt[first_id],\\\n",
    "                                basta_time_dt[last_id],color='red',alpha=0.5)               \n",
    "            \n",
    "            red_patch = mpatches.Patch(color='red',alpha=0.5,label='CEIL obscured')\n",
    "            lgnd = ax_interp_nn.legend(handles=[red_patch],\\\n",
    "                                fontsize=Fontsize*1.5,\\\n",
    "                                bbox_to_anchor=(1,1.2),\\\n",
    "                                ncol=1,loc='upper right',framealpha=0)\n",
    "\n",
    "            if False:\n",
    "                # Cubic Interpolation\n",
    "                x=ceil_time_ts = np.array([toTimestamp(ceil_time_dt[dd]) for dd in range(len(ceil_time_dt))])\n",
    "                y=ceil_range\n",
    "                #mask invalid values\n",
    "                z = ceil_backscatter.copy()\n",
    "                dumid = np.where(~np.isnan(z))\n",
    "                z[dumid] = 10.**z[dumid]\n",
    "                dumid = np.where(np.isnan(z))\n",
    "                z[dumid] = 0.\n",
    "                z = z.T\n",
    "                xx, yy = np.meshgrid(x, y)\n",
    "                basta_height_lim = basta_height[basta_height < np.max(ceil_range)]\n",
    "                newy = basta_height_lim\n",
    "                newx = basta_time_ts\n",
    "                newX,newY = np.meshgrid(newx,newy)\n",
    "                ceil_backscatter_interp_cubic1 = griddata((xx.ravel(), yy.ravel()), z.ravel(),(newX, newY),method='cubic',fill_value=np.nan)\n",
    "                dumid = np.where(~np.isnan(ceil_backscatter_interp_cubic1))\n",
    "                ceil_backscatter_interp_cubic1[dumid] = np.log10(ceil_backscatter_interp_cubic1[dumid])\n",
    "\n",
    "\n",
    "                basta_height_bins = np.arange(0,np.max(basta_height_lim),25)\n",
    "                dumbins = np.array([np.max(basta_height_lim)+12.5])\n",
    "                basta_height_bins = np.concatenate((basta_height_bins,dumbins))\n",
    "                interp_cubic1_plot = ax_interp_cubic1.pcolormesh(basta_time_dt,\\\n",
    "                                                                 basta_height_bins*1.e-3,\\\n",
    "                                                                 ceil_backscatter_interp_cubic1[:,1:],\\\n",
    "                                                                 cmap=cmap,\n",
    "                                                                 vmin=-8,vmax=-3)            \n",
    "\n",
    "                # Colorbar\n",
    "                dum_ticks = [-8,-7,-6,-5,-4,-3]\n",
    "                interp_cubic1_cbar = fig.colorbar(interp_cubic1_plot,ticks=dum_ticks,pad=0.01,ax=ax_interp_cubic1)\n",
    "                dumstr = '$log_{10}$($\\\\beta_{att}$)'\n",
    "                interp_cubic1_cbar.ax.set_ylabel(dumstr,fontsize=Fontsize)\n",
    "                interp_cubic1_cbar.ax.tick_params(labelsize=Fontsize)  \n",
    "\n",
    "                ax_interp_cubic1.set_title('Cubic Interpolation $\\\\beta_{att}$ \\n 25-m x 12-sec resolution',fontsize=Fontsize*1.5,color='dimgrey')            \n",
    "\n",
    "\n",
    "                # Linear Interpolation\n",
    "                x=ceil_time_ts = np.array([toTimestamp(ceil_time_dt[dd]) for dd in range(len(ceil_time_dt))])\n",
    "                y=ceil_range\n",
    "                #mask invalid values\n",
    "                z = ceil_backscatter.copy()\n",
    "                dumid = np.where(~np.isnan(z))\n",
    "                z[dumid] = 10.**z[dumid]\n",
    "                dumid = np.where(np.isnan(z))\n",
    "                z[dumid] = 0.\n",
    "                z = z.T\n",
    "                xx, yy = np.meshgrid(x, y)\n",
    "                basta_height_lim = basta_height[basta_height < np.max(ceil_range)]\n",
    "                newy = basta_height_lim\n",
    "                newx = basta_time_ts\n",
    "                newX,newY = np.meshgrid(newx,newy)\n",
    "                ceil_backscatter_interp_linear = griddata((xx.ravel(), yy.ravel()), z.ravel(),(newX, newY),method='linear',fill_value=np.nan)\n",
    "                dumid = np.where(~np.isnan(ceil_backscatter_interp_linear))\n",
    "                ceil_backscatter_interp_linear[dumid] = np.log10(ceil_backscatter_interp_linear[dumid])\n",
    "\n",
    "\n",
    "                basta_height_bins = np.arange(0,np.max(basta_height_lim),25)\n",
    "                dumbins = np.array([np.max(basta_height_lim)+12.5])\n",
    "                basta_height_bins = np.concatenate((basta_height_bins,dumbins))\n",
    "                interp_linear_plot = ax_interp_linear.pcolormesh(basta_time_dt,\\\n",
    "                                                                 basta_height_bins*1.e-3,\\\n",
    "                                                                 ceil_backscatter_interp_linear[:,1:],\\\n",
    "                                                                 cmap=cmap,\n",
    "                                                                 vmin=-8,vmax=-3)         \n",
    "                \n",
    "\n",
    "\n",
    "                # Colorbar\n",
    "                dum_ticks = [-8,-7,-6,-5,-4,-3]\n",
    "                interp_linear_cbar = fig.colorbar(interp_linear_plot,ticks=dum_ticks,pad=0.01,ax=ax_interp_linear)\n",
    "                dumstr = '$log_{10}$($\\\\beta_{att}$)'\n",
    "                interp_linear_cbar.ax.set_ylabel(dumstr,fontsize=Fontsize)\n",
    "                interp_linear_cbar.ax.tick_params(labelsize=Fontsize)  \n",
    "\n",
    "                ax_interp_linear.set_title('Linear Interpolation $\\\\beta_{att}$ \\n 25-m x 12-sec resolution',fontsize=Fontsize*1.5,color='dimgrey')                             \n",
    "            \n",
    "\n",
    "            #-------------------------------------\n",
    "            #-------------------------------------\n",
    "            # Fog ID\n",
    "            #-------------------------------------\n",
    "            #-------------------------------------\n",
    "            # For interpolated data, we want to come down from cloud base\n",
    "            # and determine whether or not there is a decade decrease in\n",
    "            # magnitude before reaching the lowest bin.\n",
    "            \n",
    "            fog_mask = np.zeros(np.shape(basta_time_dt))\n",
    "            min_diff_arr_out = np.zeros(np.shape(basta_time_dt))\n",
    "            for tt in range(len(basta_time_dt)):\n",
    "                if ceil_cbh_1_interp[tt] > 0.:\n",
    "                    height_id = np.where(basta_height == ceil_cbh_1_interp[tt])[0][0]\n",
    "                    cbh_beta = ceil_backscatter_interp_nn[height_id,tt]\n",
    "                    below_cbh_beta = ceil_backscatter_interp_nn[:height_id,tt]\n",
    "                    min_below_cbh_beta = np.nanmin(below_cbh_beta)\n",
    "                    dumdiff = cbh_beta - min_below_cbh_beta\n",
    "                    min_diff_arr_out[tt] = dumdiff\n",
    "                    if cbh_beta < -4.5:\n",
    "                        continue\n",
    "                    if np.isnan(dumdiff):\n",
    "                        continue\n",
    "                    elif (~np.isnan(dumdiff)) & (dumdiff >= 1.):\n",
    "                        continue\n",
    "                    elif (~np.isnan(dumdiff)) & (dumdiff < 1.):\n",
    "                        fog_mask[tt] = 1\n",
    "                else:\n",
    "                    min_diff_arr_out[tt] = np.nan\n",
    "                    \n",
    "            # Shade fog periods\n",
    "            # Fill in fog time periods with transparent blue\n",
    "            fog_id = np.where(fog_mask == 1.)\n",
    "            if np.size(fog_id) > 1.:\n",
    "                fog_Objects,num_fog_objects = ndimage.label(fog_mask)\n",
    "                for kk in range(num_fog_objects):\n",
    "                    dumid = np.where(fog_Objects == kk+1)[0]\n",
    "                    first_id = dumid[0]\n",
    "                    last_id = dumid[-1]\n",
    "                    ax_interp_nn.axvspan(basta_time_dt[first_id],\\\n",
    "                                basta_time_dt[last_id],color='navy',alpha=0.5)               \n",
    "                   \n",
    "            \n",
    "            blue_patch = mpatches.Patch(color='navy',alpha=0.5,label='Fog')\n",
    "            lgnd2 = ax_interp_nn.legend(handles=[blue_patch],\\\n",
    "                                fontsize=Fontsize*1.5,\\\n",
    "                                bbox_to_anchor=(0,1.2),\\\n",
    "                                ncol=1,loc='upper left',framealpha=0)                        \n",
    "            \n",
    "            \n",
    "            ax_interp_nn.add_artist(lgnd)\n",
    "            plt.subplots_adjust(hspace=0.3,wspace=0.1)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        #================================================\n",
    "        # Interpolate backscatter to radar grid\n",
    "        #================================================\n",
    "        if interp_backscatter:\n",
    "            basta_time_dt = np.array([toDatetime(basta_time_ts[dd]) for dd in range(len(basta_time_ts))])             \n",
    "            start_time = datetime.datetime(basta_time_dt[0].year,basta_time_dt[0].month,basta_time_dt[0].day,0,0)\n",
    "            end_time = start_time + datetime.timedelta(hours=24)          \n",
    "\n",
    "            ceil_time_dt = np.array(ceil_time_dt)\n",
    "            ceil_time_dt_orig = ceil_time_dt.copy()\n",
    "            ceil_time_ts_orig = ceil_time_ts.copy()\n",
    "            ceil_backscatter_orig = ceil_backscatter.copy()\n",
    "\n",
    "            dumid = np.where( (ceil_time_ts >= basta_time_ts[0]) & (ceil_time_ts <= basta_time_ts[-1]))\n",
    "            if np.size(dumid) > 0.:\n",
    "                dumid = np.squeeze(dumid)\n",
    "                ceil_time_ts = ceil_time_ts[dumid]\n",
    "                ceil_time_dt = ceil_time_dt[dumid]\n",
    "                ceil_backscatter = ceil_backscatter[dumid,:]        \n",
    "\n",
    "            # Interpolated in log10 space (nearest neighbor)            \n",
    "            x=ceil_time_ts = np.array([toTimestamp(ceil_time_dt[dd]) for dd in range(len(ceil_time_dt))])\n",
    "            y=ceil_range\n",
    "            #mask invalid values\n",
    "            z = ceil_backscatter.copy()\n",
    "            z = z.T\n",
    "            xx, yy = np.meshgrid(x, y)\n",
    "            basta_height_lim = basta_height[basta_height < np.max(ceil_range)]\n",
    "            newy = basta_height_lim\n",
    "            newx = basta_time_ts\n",
    "\n",
    "            newX,newY = np.meshgrid(newx,newy)\n",
    "            ceil_backscatter_interp = griddata((xx.ravel(), yy.ravel()), z.ravel(),(newX, newY),method='nearest',fill_value=np.nan)\n",
    "            \n",
    "            dumid = np.where( (basta_time_ts < ceil_time_ts_orig[0]) | (basta_time_ts > ceil_time_ts_orig[-1]) )\n",
    "            if np.size(dumid) > 0.:\n",
    "                dumid = np.squeeze(dumid)\n",
    "                ceil_cbh_1_interp[dumid] = np.nan\n",
    "                ceil_cbh_2_interp[dumid] = np.nan\n",
    "                ceil_cbh_3_interp[dumid] = np.nan\n",
    "                ceil_backscatter_interp[:,dumid] = np.nan\n",
    "                ceil_detection_status_interp[dumid] = np.nan\n",
    "                ceil_cbh_bin_relative_interp[dumid] = np.nan\n",
    "            # Need to deal with missing times in between the start and end time of the radar\n",
    "            # Let's loop through basta times and if the time is not within 12 seconds of a\n",
    "            # radar profile, we'll nan it out\n",
    "            for dum_tt in range(len(basta_time_dt)):\n",
    "                dum_diff = np.abs(basta_time_ts[dum_tt] - ceil_time_ts)\n",
    "                if np.min(dum_diff) > 12.:\n",
    "                    ceil_backscatter_interp[:,dum_tt] = np.nan\n",
    "        else:\n",
    "                ceil_backscatter_interp = None\n",
    "                basta_height_lim = None            \n",
    "\n",
    "            \n",
    "        \n",
    "        arm_ceil_out_dict = {'cbh_1':ceil_cbh_1_interp,\\\n",
    "                    'cbh_2':ceil_cbh_2_interp,\\\n",
    "                    'cbh_3':ceil_cbh_3_interp,\\\n",
    "                    'detection_status':ceil_detection_status_interp,\\\n",
    "                    'backscatter':ceil_backscatter_interp,\\\n",
    "                    'native_cbh_1':ceil_cbh_1,\\\n",
    "                    'native_cbh_2':ceil_cbh_2,\\\n",
    "                    'native_cbh_3':ceil_cbh_3,\\\n",
    "                    'native_detection_status':ceil_detection_status,\\\n",
    "                    'native_backscatter':ceil_backscatter_orig,\\\n",
    "                    'native_time_dt':ceil_time_dt_orig,\\\n",
    "                    'native_time_ts':ceil_time_ts_orig,\\\n",
    "                    'native_range':ceil_range,\\\n",
    "                    'native_range_bounds':ceil_range_bounds,\\\n",
    "                    'interp_height':basta_height_lim,\\\n",
    "                    'cbh_bin_relative_interp':ceil_cbh_bin_relative_interp}\n",
    "     \n",
    "        return arm_ceil_present_flag,arm_ceil_out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "200a9fab-1d86-43bc-839e-34b5ecf44cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_ceil(aad_ceil_dict,arm_ceil_dict,basta_time_dt):\n",
    "\n",
    "    arm_cbh_1 = arm_ceil_dict['cbh_1']\n",
    "    arm_cbh_2 = arm_ceil_dict['cbh_2']\n",
    "    arm_cbh_3 = arm_ceil_dict['cbh_3']\n",
    "    arm_detection_status = arm_ceil_dict['detection_status']\n",
    "    arm_cbh_bin_relative_interp = arm_ceil_dict['cbh_bin_relative_interp']\n",
    "\n",
    "    aad_cbh_1 = aad_ceil_dict['cbh_1']\n",
    "    aad_cbh_2 = aad_ceil_dict['cbh_2']\n",
    "    aad_cbh_3 = aad_ceil_dict['cbh_3']\n",
    "    aad_detection_status = aad_ceil_dict['detection_status']\n",
    "    aad_cbh_bin_relative_interp = aad_ceil_dict['cbh_bin_relative_interp']    \n",
    "    \n",
    "    #merge_cbh_1 = np.zeros(np.shape(basta_time_dt))\n",
    "    #merge_cbh_2 = np.zeros(np.shape(basta_time_dt))\n",
    "    #merge_cbh_3 = np.zeros(np.shape(basta_time_dt))\n",
    "    #merge_detection_status = np.zeros(np.shape(basta_time_dt))\n",
    "    #merge_cbh_bin_relative_interp = np.zeros(np.shape(basta_time_dt))\n",
    "    #merge_source_ceil = np.zeros(np.shape(basta_time_dt))\n",
    "    \n",
    "    merge_cbh_1 = arm_cbh_1.copy()\n",
    "    merge_cbh_2 = arm_cbh_2.copy()\n",
    "    merge_cbh_3 = arm_cbh_3.copy()\n",
    "    merge_detection_status = arm_detection_status.copy()\n",
    "    merge_cbh_bin_relative_interp = arm_cbh_bin_relative_interp.copy()\n",
    "    merge_source_ceil = np.zeros(np.shape(basta_time_dt))\n",
    "    merge_source_ceil[:] = 1\n",
    "    \n",
    "    # Anything with ceil_detection_status == NaN means that the ceilometer\n",
    "    # did not have a reading within 16 seconds or that the ceilometer value was\n",
    "    # generally bad. \n",
    "    #\n",
    "    # If one ceilometer exists at this time step but the other doesn't, then the merge\n",
    "    # ceilometer will use the ceilometer that DOES exist\n",
    "    dumid_nan = np.where(np.isnan(arm_detection_status) & np.isnan(aad_detection_status))\n",
    "    dumid_aad = np.where( np.isnan(arm_detection_status) & ~np.isnan(aad_detection_status) )\n",
    "    dumid_arm = np.where( ~np.isnan(arm_detection_status) & np.isnan(aad_detection_status) )\n",
    "    dumid_id4 = np.where( (arm_detection_status == 4.) & (aad_detection_status != 4.) & ~np.isnan(aad_detection_status) )\n",
    "    \n",
    "    \n",
    "    # ARM Ceilometer is obscured but AAD Ceilometer is not\n",
    "    if np.size(dumid_id4) > 0.:\n",
    "        dumid_id4 = np.squeeze(dumid_id4)\n",
    "        merge_cbh_bin_relative_interp[dumid_id4] = aad_cbh_bin_relative_interp[dumid_id4]\n",
    "        merge_cbh_1[dumid_id4] = aad_cbh_1[dumid_id4]\n",
    "        merge_cbh_2[dumid_id4] = aad_cbh_2[dumid_id4]\n",
    "        merge_cbh_3[dumid_id4] = aad_cbh_3[dumid_id4]\n",
    "        merge_detection_status[dumid_id4] = aad_detection_status[dumid_id4]\n",
    "        merge_source_ceil[dumid_id4] = 2\n",
    "    \n",
    "    # No ceilometer at all\n",
    "    if np.size(dumid_nan) > 0.:\n",
    "        dumid_nan = np.squeeze(dumid_nan)\n",
    "        merge_cbh_bin_relative_interp[dumid_nan] = np.nan\n",
    "        merge_cbh_1[dumid_nan] = np.nan\n",
    "        merge_cbh_2[dumid_nan] = np.nan\n",
    "        merge_cbh_3[dumid_nan] = np.nan\n",
    "        merge_detection_status[dumid_nan] = np.nan\n",
    "        merge_source_ceil[dumid_nan] = np.nan\n",
    "        \n",
    "    #if np.size(dumid_999) > 0.:\n",
    "    #    dumid_999 = np.squeeze(dumid_999)\n",
    "    #    merge_cbh_bin_relative_interp[dumid_999] = np.nan\n",
    "    #    merge_detection_status[dumid_999] = np.nan\n",
    "    #    merge_cbh_1[dumid_999] = np.nan\n",
    "    #    merge_cbh_2[dumid_999] = np.nan\n",
    "    #    merge_cbh_3[dumid_999] = np.nan\n",
    "    #    merge_source_ceil[dumid_999] = np.nan\n",
    "    \n",
    "    #if False:\n",
    "    if True:\n",
    "        if np.size(dumid_aad) > 0.:\n",
    "            dumid_aad = np.squeeze(dumid_aad)\n",
    "            merge_cbh_bin_relative_interp[dumid_aad] = aad_cbh_bin_relative_interp[dumid_aad]\n",
    "            merge_detection_status[dumid_aad] = aad_detection_status[dumid_aad]\n",
    "            merge_cbh_1[dumid_aad] = aad_cbh_1[dumid_aad]\n",
    "            merge_cbh_2[dumid_aad] = aad_cbh_2[dumid_aad]\n",
    "            merge_cbh_3[dumid_aad] = aad_cbh_3[dumid_aad]\n",
    "            merge_source_ceil[dumid_aad] = 2\n",
    "    if False:\n",
    "        if np.size(dumid_arm) > 0.:\n",
    "            dumid_arm = np.squeeze(dumid_arm)\n",
    "            merge_cbh_bin_relative_interp[dumid_arm] = arm_cbh_bin_relative_interp[dumid_arm]\n",
    "            merge_detection_status[dumid_arm] = arm_detection_status[dumid_arm]\n",
    "            merge_cbh_1[dumid_arm] = arm_cbh_1[dumid_arm]\n",
    "            merge_cbh_2[dumid_arm] = arm_cbh_2[dumid_arm]\n",
    "            merge_cbh_3[dumid_arm] = arm_cbh_3[dumid_arm]    \n",
    "            merge_source_ceil[dumid_arm] = 1\n",
    "        \n",
    "\n",
    "    merge_ceil_out_dict = {'detection_status':merge_detection_status,\\\n",
    "                           'cbh_bin_relative_interp':merge_cbh_bin_relative_interp,\\\n",
    "                           'cbh_1':merge_cbh_1,\\\n",
    "                           'cbh_2':merge_cbh_2,\\\n",
    "                           'cbh_3':merge_cbh_3,\\\n",
    "                           'source_ceil':merge_source_ceil,\\\n",
    "                          }\n",
    "    \n",
    "    return merge_ceil_out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b02ab40-f6ad-4853-a24b-921e056d47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #def process_native_sondes(date,sonde_dates_dt,sonde_times_dt,sonde_files,cluster_ids,cluster_times_dt):\n",
    "def process_native_sondes(date,sonde_dates_dt,sonde_times_dt,sonde_files,cluster_ids,cluster_times_dt):\n",
    "#     #,sonde_dates_dt,sonde_times_dt,sonde_files,cluster_ids,cluster_times_dt):\n",
    "    \n",
    "    sonde_id = np.where(sonde_dates_dt == date)[0]\n",
    "#     # Pull out 3 sounding times (if available) to be included in the output file.\n",
    "    num_soundings = len(sonde_times_dt)\n",
    "#     #date = date[0]\n",
    "    target_time_1 = datetime.datetime(date.year,date.month,date.day,0)\n",
    "    target_time_2 = datetime.datetime(date.year,date.month,date.day,12)\n",
    "    tmp_time_delta = datetime.timedelta(days=1)\n",
    "    target_time_3 = datetime.datetime(date.year,date.month,date.day,0) + tmp_time_delta\n",
    "    sonde_times_dt = np.array(sonde_times_dt)\n",
    "    \n",
    "    \n",
    "    tmpid = np.where((sonde_times_dt > (target_time_1 - datetime.timedelta(hours=2))) & (sonde_times_dt < (target_time_1 + datetime.timedelta(hours=2))))\n",
    "    if np.size(tmpid) > 0:\n",
    "        sonde_1_id = tmpid[0][0]\n",
    "        sonde_1_flag = True\n",
    "    else:\n",
    "        sonde_1_flag = False\n",
    "        sonde_1_id = -999.\n",
    "    tmpid = np.where((sonde_times_dt > (target_time_2 - datetime.timedelta(hours=2))) & (sonde_times_dt < (target_time_2 + datetime.timedelta(hours=2))))\n",
    "    if np.size(tmpid) > 0:\n",
    "        sonde_2_id = tmpid[0][0]\n",
    "        sonde_2_flag = True\n",
    "    else:\n",
    "        sonde_2_flag = False\n",
    "        sonde_2_id = -999.\n",
    "    tmpid = np.where((sonde_times_dt > (target_time_3 - datetime.timedelta(hours=2))) & (sonde_times_dt < (target_time_3 + datetime.timedelta(hours=2))))\n",
    "    if np.size(tmpid) > 0:\n",
    "        sonde_3_id = tmpid[0][0]\n",
    "        sonde_3_flag = True\n",
    "    else:\n",
    "        sonde_3_flag = False \n",
    "        sonde_3_id = -999.\n",
    "    \n",
    "    sonde_flags = [sonde_1_flag,sonde_2_flag,sonde_3_flag]\n",
    "    sonde_ids = [sonde_1_id,sonde_2_id,sonde_3_id]    \n",
    "\n",
    "\n",
    "    native_sonde_dict = {}\n",
    "    for jj in range(len(sonde_flags)):\n",
    "        if not sonde_flags[jj]:\n",
    "            native_sonde_dict[str(int(jj+1))] = None\n",
    "            continue\n",
    "        elif sonde_flags[jj]:\n",
    "            current_sonde_id = sonde_ids[jj]\n",
    "            current_sonde_file = sonde_files[current_sonde_id]\n",
    "            current_sonde_time = sonde_times_dt[current_sonde_id]\n",
    "            dumid = np.where(cluster_times_dt == current_sonde_time)\n",
    "            if np.size(dumid) > 0.:\n",
    "                current_cluster_id = cluster_ids[dumid[0][0]]\n",
    "            else:\n",
    "                current_cluster_id = None\n",
    "            # Calculate everything\n",
    "            current_sonde_file = current_sonde_file.split('/')[-1]\n",
    "            path = '/mnt/raid/mwstanfo/micre_soundings/'\n",
    "            file_size = os.stat(path+current_sonde_file).st_size/1.e3\n",
    "            fstruct = fs(current_sonde_file,path,file_size)\n",
    "            Sondetmp = load_sonde_data(fstruct)\n",
    "            \n",
    "            #['site_lat', 'site_lon', 'sfc_pressure', 'sfc_temp', 'sfc_humidity',\\\n",
    "            # 'sfc_wind_speed', 'sfc_wind_direction', 'dewpoint_temp', 'drybulb_temp', \\\n",
    "            # 'RH', 'pressure', 'wind_direction', 'u_wind', 'v_wind', 'wind_speed', \\\n",
    "            # 'ascent_rate', 'lat', 'lon', 'alt', 'time', 'units', 'long_name']            \n",
    "\n",
    "            max_alt = np.max(Sondetmp['alt'])\n",
    "            if max_alt < 10.:\n",
    "                print('Sonde failed to reach 10 km. Therefore omitting this sounding.')\n",
    "                native_sonde_dict[str(int(jj+1))] = None\n",
    "                sonde_flags[jj] = False\n",
    "                continue\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "#             # Calculate EIS, LTS, and LCL\n",
    "            Moretmp = calculate_theta_and_more(Sondetmp['drybulb_temp'],Sondetmp['pressure'],\\\n",
    "                                           RH=Sondetmp['RH'],use_T_K=True,\\\n",
    "                                          sat_pres_formula='Emmanuel')\n",
    "            \n",
    "            nearest_val_700hpa,nearest_id_700hpa = find_nearest(Sondetmp['pressure'],700)\n",
    "            theta_700hpa = Moretmp['Theta'][nearest_id_700hpa] # in K\n",
    "            temp_700hpa = Sondetmp['drybulb_temp'][nearest_id_700hpa] # in K\n",
    "            z_700hpa = Sondetmp['alt'][nearest_id_700hpa]*1.e3 # in meters\n",
    "            tmp_sfc_temp = Sondetmp['drybulb_temp'][0] # in K\n",
    "            tmp_sfc_pres = Sondetmp['pressure'][0] # in hPa\n",
    "            tmp_sfc_rh = Sondetmp['RH'][0] # in %\n",
    "\n",
    "            \n",
    "\n",
    "            sfc_Moretmp = calculate_theta_and_more([tmp_sfc_temp],[tmp_sfc_pres],\\\n",
    "                                       RH=[tmp_sfc_rh],use_T_K=True,\\\n",
    "                                      sat_pres_formula='Emmanuel')\n",
    "\n",
    "            tmp_sfc_theta = sfc_Moretmp['Theta'][0]\n",
    "            lts = theta_700hpa - tmp_sfc_theta\n",
    "            R_d = 287.058; # gas constant for dry air [J/(kg*K)].\n",
    "            R_v = 461.5; # gas constant for water vapor [J/(kg*K)].\n",
    "            c_p = 1005.7; # +- 2.5 [J/(kg*K)] - specific heat capacity of dry air at 273K in a constant pressure.\n",
    "            g = 9.81 # gravitational acceleration, m/s^2\n",
    "            \n",
    "            dum_T = Sondetmp['drybulb_temp']\n",
    "            dum_qs = Moretmp['q']*1.e-3\n",
    "            dum_Lv = Moretmp['L_v']\n",
    "            moist_ad_lapse_rate = (g/c_p) * (1 - ( (1 + ( (dum_Lv*dum_qs) / (R_d*dum_T) ) ) / (1 + ( ((dum_Lv**2.)*dum_qs) / (c_p*R_v*(dum_T**2.)) ) ) ) )\n",
    "            # #moist_ad_lapse_rate = (g/c_p) * (1 - ( (1+((Moretmp['L_v']*Moretmp['w_s'])/(R_d*Sondetmp['drybulb_temp']))) / (1 + (((Moretmp['L_v']**2.)*Moretmp['w_s'])/(c_p*R_v*(Sondetmp['drybulb_temp']**2.)))) ) ) # K/m\n",
    "            # #plt.plot(moist_ad_lapse_rate*1.e3,Sondetmp['alt'])\n",
    "            nearest_val_850hpa,nearest_id_850hpa = find_nearest(Sondetmp['pressure'],850)\n",
    "            # #sfc_700hpa_temp_avg = (tmp_sfc_temp + temp_700hpa)/2.\n",
    "            moist_ad_lapse_rate_850 = moist_ad_lapse_rate[nearest_id_850hpa] # K/m\n",
    "            # #moist_ad_lapse_rate_850 = moist_ad_lapse_rate_850*1.e3 # K/km\n",
    "            #pressure_metpy = Sondetmp['pressure'] * units.hPa\n",
    "            # temperature_metpy = Sondetmp['drybulb_temp'] * units.K\n",
    "            # rh_metpy = Sondetmp['RH'] * units.percent\n",
    "            # dewpoint_metpy = mpcalc.dewpoint_from_relative_humidity(temperature_metpy, rh_metpy)\n",
    "            # lcl_pres,lcl_temp = mpcalc.lcl(pressure_metpy[0],temperature_metpy[0],dewpoint_metpy[0])\n",
    "            # nearest_lcl_pres,nearest_lcl_pres_id = find_nearest(np.array(pressure_metpy),np.array(lcl_pres))\n",
    "            # lcl_z = Sondetmp['alt'][nearest_lcl_pres_id]*1.e3 # m\n",
    "            # eis = lts - moist_ad_lapse_rate_850*(z_700hpa-lcl_z)\n",
    "            \n",
    "#             if False:\n",
    "#                 print('eis:',eis,'K')\n",
    "#                 print('lts:',lts,'K')\n",
    "#                 print('lcl_z:',lcl_z,'m')\n",
    "#                 print('z_700hpa:',z_700hpa,'m')\n",
    "#                 print('moist_ad_lapse_rate_850:',moist_ad_lapse_rate_850,'K/m')\n",
    "#                 print(' ')\n",
    "#             #print(aaa)\n",
    "            \n",
    "            \n",
    "            native_sonde_dict[str(int(jj+1))] = {}\n",
    "            native_sonde_dict[str(int(jj+1))]['temperature'] = Sondetmp['drybulb_temp']\n",
    "            native_sonde_dict[str(int(jj+1))]['pressure'] = Sondetmp['pressure']\n",
    "            native_sonde_dict[str(int(jj+1))]['rh'] = Sondetmp['RH']\n",
    "            native_sonde_dict[str(int(jj+1))]['u'] = Sondetmp['u_wind']\n",
    "            native_sonde_dict[str(int(jj+1))]['v'] = Sondetmp['v_wind']\n",
    "            native_sonde_dict[str(int(jj+1))]['wind_speed'] = Sondetmp['wind_speed']\n",
    "            native_sonde_dict[str(int(jj+1))]['wind_dir'] = Sondetmp['wind_direction']\n",
    "            native_sonde_dict[str(int(jj+1))]['height'] = Sondetmp['alt']*1.e3\n",
    "            native_sonde_dict[str(int(jj+1))]['time_dt_long'] = Sondetmp['time']\n",
    "\n",
    "\n",
    "            native_sonde_dict[str(int(jj+1))]['q'] = Moretmp['q']\n",
    "            native_sonde_dict[str(int(jj+1))]['theta'] = Moretmp['Theta']\n",
    "            native_sonde_dict[str(int(jj+1))]['theta_e'] = Moretmp['Theta_e']\n",
    "            native_sonde_dict[str(int(jj+1))]['rh_i'] = Moretmp['RH_i']\n",
    "            native_sonde_dict[str(int(jj+1))]['w_s'] = Moretmp['L_v']\n",
    "            native_sonde_dict[str(int(jj+1))]['e'] = Moretmp['e']\n",
    "            native_sonde_dict[str(int(jj+1))]['time_dt'] = current_sonde_time\n",
    "            native_sonde_dict[str(int(jj+1))]['cluster_id'] = current_cluster_id\n",
    "#             native_sonde_dict[str(int(jj+1))]['eis'] = eis\n",
    "#             native_sonde_dict[str(int(jj+1))]['lts'] = lts\n",
    "#             native_sonde_dict[str(int(jj+1))]['lcl'] = lcl_z\n",
    "\n",
    "    if False:        \n",
    "        for jj in range(len(sonde_flags)):\n",
    "            if sonde_flags[jj]:\n",
    "                fig = plt.figure(figsize=(18,9))\n",
    "                dum_keys = ['temperature','rh','rh_i','theta','theta_e','pressure','wind_speed','wind_dir','u','v','q','w_s','e']\n",
    "                for ii in range(len(dum_keys)):\n",
    "                    dumax = fig.add_subplot(2,8,ii+1)\n",
    "                    dumax.plot(native_sonde_dict[str(int(jj+1))][dum_keys[ii]],native_sonde_dict[str(int(jj+1))]['height']*1.e-3,c='k',lw=2)\n",
    "                    dumax.set_ylabel('Height [km]',fontsize=14)\n",
    "                    dumax.set_xlabel(dum_keys[ii],fontsize=14)\n",
    "                    dumax.tick_params(labelsize=14)\n",
    "                    dumax.grid(True)\n",
    "                    plt.subplots_adjust(wspace=0.5,top=0.925)\n",
    "                    plt.suptitle(native_sonde_dict[str(int(jj+1))]['time_dt'],fontsize=24)\n",
    "                plt.show()\n",
    "                plt.close()\n",
    "    # We have a \"native\" sonde dict with the original soundings\n",
    "    # Now calculate a few things\n",
    "    if np.all(sonde_flags is not False):\n",
    "        sonde_present_flag = True\n",
    "    else:\n",
    "        sonde_present_flag = False\n",
    "    #return sonde_present_flag, native_sonde_dict\n",
    "    return sonde_present_flag, native_sonde_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b40b9c8-7bfc-459a-9e40-e682e4fb2bec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#==================================================\n",
    "# Main function to merge instruments\n",
    "#\n",
    "# Inputs: date -- from basta_dates_dt\n",
    "#         avg_bool -- boolean to perform averaging\n",
    "#==================================================\n",
    "#def merge_instruments(date,basta_files,basta_dates_dt,arm_ceil_dates_dt,arm_files,aad_ceil_dates_dt,aad_ceil_files,sonde_dates_dt,sonde_files):\n",
    "def merge_instruments(date,props_dict):\n",
    "    \n",
    "    avg_bool = False\n",
    "    basta_files = props_dict['basta_files']\n",
    "    basta_dates_dt = props_dict['basta_dates_dt']\n",
    "    arm_ceil_files = props_dict['arm_ceil_files']\n",
    "    arm_ceil_dates_dt = props_dict['arm_ceil_dates_dt']\n",
    "    aad_ceil_files = props_dict['aad_ceil_files']\n",
    "    aad_ceil_dates_dt = props_dict['aad_ceil_dates_dt']\n",
    "    sonde_files = props_dict['sonde_files']\n",
    "    sonde_dates_dt = props_dict['sonde_dates_dt']\n",
    "    sonde_times_dt = props_dict['sonde_times_dt']\n",
    "    cluster_ids = props_dict['cluster_ids']\n",
    "    cluster_times_dt = props_dict['cluster_times_dt']\n",
    "    \n",
    "    print('-------------------------------------------------')\n",
    "    print('-------------------------------------------------')\n",
    "    print('Date: ',date.strftime('%Y/%m/%d'))\n",
    "    print('-------------------------------------------------')\n",
    "    print('-------------------------------------------------')\n",
    "    \n",
    "\n",
    "#     # Dictionary that will hold all merged data, separated by intrument\n",
    "    merged_dict = {}\n",
    "\n",
    "#     # Process BASTA data\n",
    "    basta_present_flag,basta_out_dict = process_basta(date,basta_dates_dt,basta_files,avg_bool)\n",
    "    if not basta_present_flag:\n",
    "        print('Bad radar data on this date. Skipping')\n",
    "        merged_dict = None\n",
    "        return basta_present_flag,merged_dict\n",
    "    else:\n",
    "         pass\n",
    "    \n",
    "    merged_dict['basta'] = basta_out_dict\n",
    "    if not basta_present_flag:\n",
    "        return basta_present_flag\n",
    "\n",
    "#     # Process AAD CEIL data\n",
    "    aad_ceil_present_flag,aad_ceil_out_dict = process_aad_ceil(date,aad_ceil_dates_dt,aad_ceil_files,avg_bool,basta_out_dict['height'],basta_out_dict['time_ts'],interp_backscatter=True)\n",
    "    if aad_ceil_present_flag:\n",
    "        merged_dict['aad_ceil'] = aad_ceil_out_dict \n",
    "\n",
    "\n",
    "#     # Process ARM CEIL data\n",
    "    arm_ceil_present_flag,arm_ceil_out_dict = process_arm_ceil(date,arm_ceil_dates_dt,arm_ceil_files,avg_bool,basta_out_dict['height'],basta_out_dict['time_ts'],interp_backscatter=True)\n",
    "    if arm_ceil_present_flag:\n",
    "        merged_dict['arm_ceil'] = arm_ceil_out_dict\n",
    "\n",
    "#     # Process Sounding data\n",
    "    #sonde_present_flag,native_sonde_out_dict = process_native_sondes(date,sonde_dates_dt,sonde_times_dt,sonde_files,cluster_ids,cluster_times_dt)\n",
    "    sonde_present_flag,native_sonde_out_dict = process_native_sondes(date,sonde_dates_dt,sonde_times_dt,sonde_files,cluster_ids,cluster_times_dt)#,sonde_dates_dt,sonde_times_dt,sonde_files,cluster_ids,cluster_times_dt)\n",
    "    if sonde_present_flag:\n",
    "        merged_dict['native_sonde'] = native_sonde_out_dict \n",
    "\n",
    "        \n",
    "        \n",
    "#     # Interp Soundings\n",
    "    interp_sonde_present_flag, interp_sonde_out_dict = interp_sondes(date,sonde_dates_dt,sonde_times_dt,sonde_files,merged_dict['basta']['time_dt'],merged_dict['basta']['height'],cluster_ids,cluster_times_dt)\n",
    "    if interp_sonde_present_flag:\n",
    "        merged_dict['interp_sonde'] = interp_sonde_out_dict\n",
    "\n",
    "\n",
    "#     # Process SFC Met\n",
    "    sfc_met_present_flag,sfc_met_out_dict = process_sfc_met(date,sfc_dates_dt,sfc_files,avg_bool,basta_out_dict['time_ts'])\n",
    "    if sfc_met_present_flag:\n",
    "        merged_dict['sfc_met'] = sfc_met_out_dict      \n",
    "\n",
    "\n",
    "#     # Process satellite\n",
    "    sat_present_flag,sat_out_dict = process_sat(date,sat_dates_dt,sat_files)\n",
    "\n",
    "    if sat_present_flag:\n",
    "        merged_dict['sat'] = sat_out_dict  \n",
    "\n",
    "        \n",
    "\n",
    "#     # Process PIRAT\n",
    "    dis_present_flag,dis_out_dict = process_dis(date,dis_dates_dt,dis_files,avg_bool,basta_out_dict['time_ts'])\n",
    "    if dis_present_flag:\n",
    "        merged_dict['dis'] = dis_out_dict\n",
    "\n",
    "        \n",
    "\n",
    "#     # Process Optics\n",
    "    optics_present_flag,optics_out_dict = process_optics(date,optics_dates_dt,optics_files,avg_bool,basta_out_dict['time_ts'])\n",
    "    if optics_present_flag:\n",
    "        merged_dict['optics'] = optics_out_dict\n",
    "        \n",
    "\n",
    "#     # Merge ceilometers (prioritize AAD ceilometer)\n",
    "    if not aad_ceil_present_flag and not arm_ceil_present_flag:\n",
    "        # If neither ceilometer is present\n",
    "        merge_ceil_out_dict = None\n",
    "        merge_ceil_present_flag = False\n",
    "    elif aad_ceil_present_flag and not arm_ceil_present_flag:\n",
    "        # if AAD is only ceilometer present\n",
    "        merge_ceil_out_dict = {}\n",
    "        merge_ceil_out_dict['cbh_1'] = merged_dict['aad_ceil']['cbh_1']\n",
    "        merge_ceil_out_dict['cbh_2'] = merged_dict['aad_ceil']['cbh_2']\n",
    "        merge_ceil_out_dict['cbh_3'] = merged_dict['aad_ceil']['cbh_3']\n",
    "        merge_ceil_out_dict['detection_status'] = merged_dict['aad_ceil']['detection_status']\n",
    "        merge_ceil_out_dict['cbh_bin_relative_interp'] = merged_dict['aad_ceil']['cbh_bin_relative_interp']\n",
    "        dum = np.zeros(np.shape(merged_dict['aad_ceil']['detection_status']))\n",
    "        dum[:] = 1\n",
    "        merge_ceil_out_dict['source_ceil'] = dum\n",
    "        merge_ceil_present_flag = True\n",
    "        merged_dict['merge_ceil'] = merge_ceil_out_dict\n",
    "    elif not aad_ceil_present_flag and arm_ceil_present_flag:\n",
    "        # if ARM is only ceilometer present\n",
    "        merge_ceil_out_dict = {}\n",
    "        merge_ceil_out_dict['cbh_1'] = merged_dict['arm_ceil']['cbh_1']\n",
    "        merge_ceil_out_dict['cbh_2'] = merged_dict['arm_ceil']['cbh_2']\n",
    "        merge_ceil_out_dict['cbh_3'] = merged_dict['arm_ceil']['cbh_3']\n",
    "        merge_ceil_out_dict['detection_status'] = merged_dict['arm_ceil']['detection_status']\n",
    "        merge_ceil_out_dict['cbh_bin_relative_interp'] = merged_dict['arm_ceil']['cbh_bin_relative_interp']\n",
    "        dum = np.zeros(np.shape(merged_dict['arm_ceil']['detection_status']))\n",
    "        dum[:] = 2\n",
    "        merge_ceil_out_dict['source_ceil'] = dum\n",
    "        merge_ceil_present_flag = True\n",
    "        merged_dict['merge_ceil'] = merge_ceil_out_dict\n",
    "    elif aad_ceil_present_flag and arm_ceil_present_flag:\n",
    "        # if both ceilometers present\n",
    "        #merge_ceil_out_dict = {}\n",
    "        #merge_ceil_out_dict['cbh_1'] = merged_dict['aad_ceil']['cbh_1']\n",
    "        #merge_ceil_out_dict['cbh_2'] = merged_dict['aad_ceil']['cbh_2']\n",
    "        #merge_ceil_out_dict['cbh_3'] = merged_dict['aad_ceil']['cbh_3']\n",
    "        #merge_ceil_out_dict['detction_status'] = merged_dict['aad_ceil']['detection_status']\n",
    "        merge_ceil_out_dict = merge_ceil(merged_dict['aad_ceil'],merged_dict['arm_ceil'],merged_dict['basta']['time_dt'])\n",
    "        merge_ceil_present_flag = True\n",
    "        merged_dict['merge_ceil'] = merge_ceil_out_dict\n",
    "        #merge_ceil_out_dict = None\n",
    "        #merged_dict['merge_ceil'] = merge_ceil_out_dict\n",
    "\n",
    "#     # Dataset Flags\n",
    "    merged_dict['dataset_flags'] = {'basta':basta_present_flag,\\\n",
    "                                    'arm_ceil':arm_ceil_present_flag,\\\n",
    "                                    'aad_ceil':aad_ceil_present_flag,\\\n",
    "                                    'sfc_met':sfc_met_present_flag,\\\n",
    "                                    'dis':dis_present_flag,\\\n",
    "                                    'sat':sat_present_flag,\\\n",
    "                                    'merge_ceil':merge_ceil_present_flag,\\\n",
    "                                    'interp_sonde':interp_sonde_present_flag,\\\n",
    "                                    'optics':optics_present_flag,\\\n",
    "                                    'native_sonde':sonde_present_flag}\n",
    "\n",
    "\n",
    "    if basta_present_flag:\n",
    "        save_path = '/mnt/raid/mwstanfo/micre/merged_instrument_files/'\n",
    "        dum_time_str = date.strftime('%Y%m%d')\n",
    "        out_pkl_file = save_path+'merged_instruments_{}_v3arm.p'.format(dum_time_str)\n",
    "        pickle.dump(merged_dict,open(out_pkl_file,\"wb\"))\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    #return merged_dict\n",
    "\n",
    "#     #return basta_present_flag,merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f0cdc6f9-24e0-49dc-baa2-9ede38a3af20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_aad_ceil(date,aad_ceil_dates_dt,aad_ceil_files,avg_bool,basta_height,basta_time_ts,interp_backscatter=False):\n",
    "    tmpid = np.where(aad_ceil_dates_dt == date)\n",
    "    if np.size(tmpid) == 0.:\n",
    "        aad_ceil_present_flag = False\n",
    "        aad_ceil_out_dict = None\n",
    "        return aad_ceil_present_flag, aad_ceil_out_dict\n",
    "    elif np.size(tmpid) > 0.:        \n",
    "        aad_ceil_present_flag = True\n",
    "        tmpid = tmpid[0][0]\n",
    "        current_ceil_file = aad_ceil_files[tmpid]\n",
    "        ncfile = xarray.open_dataset(current_ceil_file,decode_times=False)    \n",
    "        ceil_dims = ncfile.dims\n",
    "        ceil_time_ts = np.array(ncfile['time']).copy()\n",
    "        ceil_cbh_1 = np.array(ncfile['cbh_1']).copy()\n",
    "        ceil_cbh_2 = np.array(ncfile['cbh_2']).copy()\n",
    "        ceil_cbh_3 = np.array(ncfile['cbh_3']).copy()\n",
    "        ceil_detection_status = np.array(ncfile['detection_status']).copy()\n",
    "        ceil_backscatter = np.array(ncfile['backscatter']).copy()\n",
    "        ceil_height = np.array(ncfile['height']).copy()\n",
    "        ceil_num_times = ceil_dims['time_dim']\n",
    "        ceil_time_dt = np.array([toDatetime(ceil_time_ts[dd]) for dd in range(ceil_num_times)]  )  \n",
    "        ncfile.close()\n",
    "\n",
    "        #------------------------------------------\n",
    "        # Interpolate ceilometer to radar time grid\n",
    "        # using nearest neighbor interpolation. \n",
    "        # This method requires that the nearest\n",
    "        # neighbor be within 15 seconds of the\n",
    "        # radar time grid element.\n",
    "        #------------------------------------------\n",
    "\n",
    "        basta_bin_edges = np.arange(0,np.max(basta_height)+12.5+25.,25.)\n",
    "        basta_time_dt = np.array([toDatetime(basta_time_ts[dd]) for dd in range(len(basta_time_ts))])\n",
    "\n",
    "        ceil_cbh_1_interp = []\n",
    "        ceil_cbh_2_interp = []\n",
    "        ceil_cbh_3_interp = []\n",
    "        ceil_detection_status_interp = []\n",
    "        ceil_cbh_bin_relative_interp = []\n",
    "        \n",
    "        for ttt in range(len(basta_time_ts)):\n",
    "            # if here, then good radar data exists\n",
    "            # Now find the nearest in time ceilometer time step to the radar time step\n",
    "            # If the ceilometer is more than 15 seconds away from the the radar time step,\n",
    "            # then we will flag it as missing data (NaN)\n",
    "            nearest_val,nearest_id = find_nearest(ceil_time_ts,basta_time_ts[ttt])\n",
    "            time_diff = np.abs(nearest_val - basta_time_ts[ttt])\n",
    "            target_time_diff = 16\n",
    "            if time_diff <= target_time_diff:\n",
    "                nearest_ceil_cbh_1 = ceil_cbh_1[nearest_id]\n",
    "                nearest_ceil_cbh_2 = ceil_cbh_2[nearest_id]\n",
    "                nearest_ceil_cbh_3 = ceil_cbh_3[nearest_id]\n",
    "                nearest_ceil_detection_status = ceil_detection_status[nearest_id]\n",
    "                ceil_detection_status_interp.append(nearest_ceil_detection_status)\n",
    "\n",
    "                if np.isnan(nearest_ceil_detection_status):\n",
    "                    #ceil_detection_status_interp.append(np.nan)\n",
    "                    ceil_cbh_1_interp.append(np.nan)\n",
    "                    ceil_cbh_2_interp.append(np.nan)\n",
    "                    ceil_cbh_3_interp.append(np.nan)\n",
    "                    ceil_cbh_bin_relative_interp.append(np.nan)\n",
    "                    continue\n",
    "                    \n",
    "                elif (nearest_ceil_detection_status == 5.) or (nearest_ceil_detection_status == 0.):\n",
    "                    ceil_cbh_1_interp.append(-999.)\n",
    "                    ceil_cbh_2_interp.append(-999.)\n",
    "                    ceil_cbh_3_interp.append(-999.)\n",
    "                    ceil_cbh_bin_relative_interp.append(-999.)\n",
    "                    continue\n",
    "                    \n",
    "                elif (nearest_ceil_detection_status == 4.):\n",
    "                    ceil_cbh_1_interp.append(np.nan)\n",
    "                    ceil_cbh_2_interp.append(np.nan)\n",
    "                    ceil_cbh_3_interp.append(np.nan)\n",
    "                    ceil_cbh_bin_relative_interp.append(np.nan)\n",
    "                    continue                    \n",
    "                    \n",
    "                elif (nearest_ceil_detection_status == 1.) or (nearest_ceil_detection_status == 2.) or (nearest_ceil_detection_status == 3.):\n",
    "                    pass\n",
    "                else:\n",
    "                    raise RuntimeError('Something went wrong')\n",
    "\n",
    "                #if np.isnan(nearest_ceil_cbh_1):\n",
    "                #    ceil_cbh_1_interp.append(np.nan)\n",
    "                #    ceil_cbh_2_interp.append(np.nan)\n",
    "                #    ceil_cbh_3_interp.append(np.nan)\n",
    "                #    ceil_cbh_bin_relative_interp.append(np.nan)\n",
    "                #    continue \n",
    "                    \n",
    "                \n",
    "                    \n",
    "                nearest_val,nearest_id = find_nearest(basta_bin_edges,nearest_ceil_cbh_1)\n",
    "                if nearest_ceil_cbh_1 > 12000.:\n",
    "                    ceil_cbh_1_interp.append(-999.)\n",
    "                    ceil_cbh_2_interp.append(-999.)\n",
    "                    ceil_cbh_3_interp.append(-999.)\n",
    "                    ceil_cbh_bin_relative_interp.append(-999.)\n",
    "                    ceil_detection_status_interp[ttt] = 0.\n",
    "                else:\n",
    "                    if nearest_ceil_cbh_1 == nearest_val:\n",
    "                        bin_edges = basta_bin_edges[nearest_id-1:nearest_id+1]\n",
    "                        midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                        ceil_cbh_1_interp.append(midbin)\n",
    "                        ceil_cbh_bin_relative_interp.append(1.)\n",
    "                    elif nearest_ceil_cbh_1 < nearest_val:\n",
    "                        bin_edges = basta_bin_edges[nearest_id-1:nearest_id+1]\n",
    "                        midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                        ceil_cbh_1_interp.append(midbin)\n",
    "                        ceil_cbh_bin_relative_interp.append(0.)\n",
    "                    elif nearest_ceil_cbh_1 > nearest_val:\n",
    "                        bin_edges = basta_bin_edges[nearest_id:nearest_id+2]\n",
    "                        midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                        ceil_cbh_1_interp.append(midbin)\n",
    "                        ceil_cbh_bin_relative_interp.append(2.)\n",
    "                    \n",
    "                    if np.isnan(nearest_ceil_cbh_2):\n",
    "                        ceil_cbh_2_interp.append(np.nan)\n",
    "                    else:    \n",
    "                        nearest_val,nearest_id = find_nearest(basta_bin_edges,nearest_ceil_cbh_2)\n",
    "                        if nearest_ceil_cbh_2 == nearest_val:\n",
    "                            bin_edges = basta_bin_edges[nearest_id-1:nearest_id+1]\n",
    "                            midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                            ceil_cbh_2_interp.append(midbin)\n",
    "                        elif nearest_ceil_cbh_2 < nearest_val:\n",
    "                            bin_edges = basta_bin_edges[nearest_id-1:nearest_id+1]\n",
    "                            midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                            ceil_cbh_2_interp.append(midbin)\n",
    "                        elif nearest_ceil_cbh_2 > nearest_val:\n",
    "                            bin_edges = basta_bin_edges[nearest_id:nearest_id+2]\n",
    "                            midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                            ceil_cbh_2_interp.append(midbin)\n",
    "\n",
    "                    if np.isnan(nearest_ceil_cbh_3):\n",
    "                        ceil_cbh_3_interp.append(np.nan)\n",
    "                    else:                      \n",
    "                        nearest_val,nearest_id = find_nearest(basta_bin_edges,nearest_ceil_cbh_3)\n",
    "                        if nearest_ceil_cbh_3 == nearest_val:\n",
    "                            bin_edges = basta_bin_edges[nearest_id-1:nearest_id+1]\n",
    "                            midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                            ceil_cbh_3_interp.append(midbin)\n",
    "                        elif nearest_ceil_cbh_3 < nearest_val:\n",
    "                            bin_edges = basta_bin_edges[nearest_id-1:nearest_id+1]\n",
    "                            midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                            ceil_cbh_3_interp.append(midbin)\n",
    "                        elif nearest_ceil_cbh_3 > nearest_val:\n",
    "                            bin_edges = basta_bin_edges[nearest_id:nearest_id+2]\n",
    "                            midbin = (bin_edges[0]+bin_edges[1])/2.\n",
    "                            ceil_cbh_3_interp.append(midbin)                    \n",
    "                    \n",
    "                    \n",
    "                if np.isnan(ceil_cbh_1_interp[ttt]):\n",
    "                    print(aaaa)\n",
    "            else:\n",
    "                #print('here')\n",
    "                #print(time_diff,basta_time_dt[ttt],ceil_time_dt[nearest_id])\n",
    "                ceil_cbh_1_interp.append(np.nan)\n",
    "                ceil_cbh_2_interp.append(np.nan)\n",
    "                ceil_cbh_3_interp.append(np.nan)\n",
    "                ceil_detection_status_interp.append(np.nan)\n",
    "                ceil_cbh_bin_relative_interp.append(np.nan)\n",
    "\n",
    "        ceil_cbh_1_interp = np.array(ceil_cbh_1_interp)\n",
    "        ceil_cbh_2_interp = np.array(ceil_cbh_2_interp)\n",
    "        ceil_cbh_3_interp = np.array(ceil_cbh_3_interp)\n",
    "        ceil_detection_status_interp = np.array(ceil_detection_status_interp)    \n",
    "        ceil_cbh_bin_relative_interp = np.array(ceil_cbh_bin_relative_interp) \n",
    "            \n",
    "        #dumid = np.where(ceil_height <= np.max(basta_height))\n",
    "        #dumid = np.squeeze(dumid)\n",
    "        #ceil_backscatter = ceil_backscatter[:,dumid]\n",
    "        ceil_backscatter[ceil_backscatter == 0.] = np.nan\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "                \n",
    "        #============================================\n",
    "        # Plot to explore backscatter interpolation\n",
    "        #============================================\n",
    "        #if True: \n",
    "        if False:\n",
    "\n",
    "            fig = plt.figure(figsize=(24,14))\n",
    "            Fontsize=14\n",
    "            dfmt = mdates.DateFormatter('%H:%M')\n",
    "            ax_native = fig.add_subplot(211)\n",
    "            ax_interp_nn = fig.add_subplot(212)\n",
    "            \n",
    "            basta_time_dt = np.array([toDatetime(basta_time_ts[dd]) for dd in range(len(basta_time_ts))])             \n",
    "            start_time = datetime.datetime(basta_time_dt[0].year,basta_time_dt[0].month,basta_time_dt[0].day,0,0)\n",
    "            #end_time = start_time + datetime.timedelta(days=1)           \n",
    "            end_time = start_time + datetime.timedelta(hours=24)           \n",
    "            \n",
    "            axlist = [ax_native,ax_interp_nn]\n",
    "            for ax in axlist:\n",
    "                ax.tick_params(labelsize=Fontsize)\n",
    "                ax.set_ylabel('Height [km]',fontsize=Fontsize)\n",
    "                ax.set_xlabel('UTC Time [HH:MM]',fontsize=Fontsize)\n",
    "                ax.xaxis.set_major_formatter(dfmt)\n",
    "                ax.grid(which='both',c='dimgrey',ls='dotted',lw=1)\n",
    "                ax.set_xlim(start_time,end_time)\n",
    "                ax.set_ylim(0,1)\n",
    "                \n",
    "            cmap = matplotlib.cm.get_cmap(\"jet\").copy()\n",
    "            cmap.set_under('navy')\n",
    "            cmap.set_bad('grey')\n",
    "\n",
    "            # Native\n",
    "            height_bins = np.arange(0,np.max(ceil_height),10)\n",
    "            dumbin = np.array([height_bins[-1]+10])\n",
    "            height_bins = np.concatenate((height_bins,dumbin))\n",
    "\n",
    "            native_plot = ax_native.pcolormesh(ceil_time_dt,\\\n",
    "                                                             height_bins*1.e-3,\\\n",
    "                                                             ceil_backscatter[1:,:].T,\\\n",
    "                                                             cmap=cmap,\n",
    "                                                             vmin=-8,vmax=-3)\n",
    "            # Colorbar\n",
    "            dum_ticks = [-8,-7,-6,-5,-4,-3]\n",
    "            native_cbar = fig.colorbar(native_plot,ticks=dum_ticks,pad=0.01,ax=ax_native)\n",
    "            dumstr = '$log_{10}$($\\\\beta_{att}$)'\n",
    "            native_cbar.ax.set_ylabel(dumstr,fontsize=Fontsize)\n",
    "            native_cbar.ax.tick_params(labelsize=Fontsize)  \n",
    "            \n",
    "            ax_native.set_title('Native $\\\\beta_{att}$ \\n 10-m x 6-sec resolution',fontsize=Fontsize*1.5,color='dimgrey')\n",
    "            ax_native.scatter(ceil_time_dt,ceil_cbh_1*1.e-3,s=2,c='black')\n",
    "            \n",
    "            # Fill in obscured time periods with transparent red\n",
    "            id4 = np.where(ceil_detection_status == 4.)\n",
    "            detection_mask = np.zeros(np.shape(ceil_detection_status))\n",
    "            if np.size(id4) > 1.:\n",
    "                id4 = np.squeeze(id4)\n",
    "                detection_mask[id4] = 1\n",
    "                detection_Objects,num_detection_objects = ndimage.label(detection_mask)\n",
    "                for kk in range(num_detection_objects):\n",
    "                    dumid = np.where(detection_Objects == kk+1)[0]\n",
    "                    first_id = dumid[0]\n",
    "                    last_id = dumid[-1]\n",
    "                    ax_native.axvspan(ceil_time_dt[first_id],\\\n",
    "                                ceil_time_dt[last_id],color='red',alpha=0.5)               \n",
    "            \n",
    "            \n",
    "            \n",
    "            #ceil_time_dt = np.array(ceil_time_dt)\n",
    "            ceil_time_dt_orig = ceil_time_dt.copy()\n",
    "            ceil_time_ts_orig = ceil_time_ts.copy()\n",
    "            \n",
    "            dumid = np.where( (ceil_time_ts >= basta_time_ts[0]) & (ceil_time_ts <= basta_time_ts[-1]))\n",
    "            if np.size(dumid) > 0.:\n",
    "                dumid = np.squeeze(dumid)\n",
    "                ceil_time_ts = ceil_time_ts[dumid]\n",
    "                ceil_time_dt = ceil_time_dt[dumid]\n",
    "                ceil_backscatter = ceil_backscatter[dumid,:]\n",
    "                \n",
    "                \n",
    "            # Interpolated in log10 space (nearest neighbor)\n",
    "            x=ceil_time_ts = np.array([toTimestamp(ceil_time_dt[dd]) for dd in range(len(ceil_time_dt))])\n",
    "            y=ceil_height\n",
    "            #mask invalid values\n",
    "            z = ceil_backscatter.copy()\n",
    "            z = z.T\n",
    "            xx, yy = np.meshgrid(x, y)\n",
    "            #basta_height_lim = basta_height[basta_height < np.max(ceil_range)]\n",
    "            #newy = basta_height_lim\n",
    "            newy = basta_height\n",
    "            newx = basta_time_ts\n",
    "            newX,newY = np.meshgrid(newx,newy)\n",
    "            ceil_backscatter_interp_nn = griddata((xx.ravel(), yy.ravel()), z.ravel(),(newX, newY),method='nearest',fill_value=np.nan)\n",
    "            \n",
    "            dumid = np.where( (basta_time_ts < ceil_time_ts_orig[0]) | (basta_time_ts > ceil_time_ts_orig[-1]) )\n",
    "            if np.size(dumid) > 0.:\n",
    "                dumid = np.squeeze(dumid)\n",
    "                ceil_cbh_1_interp[dumid] = np.nan\n",
    "                ceil_cbh_2_interp[dumid] = np.nan\n",
    "                ceil_cbh_3_interp[dumid] = np.nan\n",
    "                ceil_backscatter_interp_nn[:,dumid] = np.nan\n",
    "                ceil_detection_status_interp[dumid] = np.nan\n",
    "                ceil_cbh_bin_relative_interp[dumid] = np.nan\n",
    "                \n",
    "            # Need to deal with missing times in between the start and end time of the radar\n",
    "            # Let's loop through basta times and if the time is not within 12 seconds of a\n",
    "            # radar profile, we'll nan it out\n",
    "\n",
    "            for dum_tt in range(len(basta_time_dt)):\n",
    "                dum_diff = np.abs(basta_time_ts[dum_tt] - ceil_time_ts)\n",
    "                if np.min(dum_diff) > 12.:\n",
    "                    ceil_backscatter_interp_nn[:,dum_tt] = np.nan\n",
    "            \n",
    "                \n",
    "            basta_height_bins = np.arange(0,np.max(basta_height),25)\n",
    "            dumbins = np.array([np.max(basta_height)+12.5])\n",
    "            basta_height_bins = np.concatenate((basta_height_bins,dumbins))\n",
    "            interp_nn_plot = ax_interp_nn.pcolormesh(basta_time_dt,\\\n",
    "                                                             basta_height_bins*1.e-3,\\\n",
    "                                                             ceil_backscatter_interp_nn[:,1:],\\\n",
    "                                                             cmap=cmap,\n",
    "                                                             vmin=-8,vmax=-3)            \n",
    "            \n",
    "            # Colorbar\n",
    "            dum_ticks = [-8,-7,-6,-5,-4,-3]\n",
    "            interp_nn_cbar = fig.colorbar(interp_nn_plot,ticks=dum_ticks,pad=0.01,ax=ax_interp_nn)\n",
    "            dumstr = '$log_{10}$($\\\\beta_{att}$)'\n",
    "            interp_nn_cbar.ax.set_ylabel(dumstr,fontsize=Fontsize)\n",
    "            interp_nn_cbar.ax.tick_params(labelsize=Fontsize)  \n",
    "            \n",
    "            ax_interp_nn.set_title('Nearest Neighbor Interpolation $\\\\beta_{att}$ \\n 25-m x 12-sec resolution',fontsize=Fontsize*1.5,color='dimgrey')            \n",
    "            # Interpolated CBH\n",
    "            ax_interp_nn.scatter(basta_time_dt,ceil_cbh_1_interp*1.e-3,s=2,c='black')\n",
    "\n",
    "            \n",
    "            # Fill in obscured time periods with transparent red\n",
    "            id4 = np.where(ceil_detection_status_interp == 4.)\n",
    "            detection_mask = np.zeros(np.shape(ceil_detection_status_interp))\n",
    "            if np.size(id4) > 1.:\n",
    "                id4 = np.squeeze(id4)\n",
    "                detection_mask[id4] = 1\n",
    "                detection_Objects,num_detection_objects = ndimage.label(detection_mask)\n",
    "                for kk in range(num_detection_objects):\n",
    "                    dumid = np.where(detection_Objects == kk+1)[0]\n",
    "                    first_id = dumid[0]\n",
    "                    last_id = dumid[-1]\n",
    "                    ax_interp_nn.axvspan(basta_time_dt[first_id],\\\n",
    "                                basta_time_dt[last_id],color='red',alpha=0.5)               \n",
    "            \n",
    "            red_patch = mpatches.Patch(color='red',alpha=0.5,label='CEIL obscured')\n",
    "            lgnd = ax_interp_nn.legend(handles=[red_patch],\\\n",
    "                                fontsize=Fontsize*1.5,\\\n",
    "                                bbox_to_anchor=(1,1.2),\\\n",
    "                                ncol=1,loc='upper right',framealpha=0)\n",
    "            \n",
    "\n",
    "            #-------------------------------------\n",
    "            #-------------------------------------\n",
    "            # Fog ID\n",
    "            #-------------------------------------\n",
    "            #-------------------------------------\n",
    "            # For interpolated data, we want to come down from cloud base\n",
    "            # and determine whether or not there is a decade decrease in\n",
    "            # magnitude before reaching the lowest bin.\n",
    "            \n",
    "            fog_mask = np.zeros(np.shape(basta_time_dt))\n",
    "            min_diff_arr_out = np.zeros(np.shape(basta_time_dt))\n",
    "            for tt in range(len(basta_time_dt)):\n",
    "                if ceil_cbh_1_interp[tt] > 0.:\n",
    "                    height_id = np.where(basta_height == ceil_cbh_1_interp[tt])#[0][0]\n",
    "                    #height_id = np.squeeze(height_id)\n",
    "                    #height_id = height_id[0]\n",
    "                    #print(np.shape(height_id))\n",
    "                    continue\n",
    "                    cbh_beta = ceil_backscatter_interp_nn[height_id,tt]\n",
    "                    below_cbh_beta = ceil_backscatter_interp_nn[:height_id,tt]\n",
    "                    min_below_cbh_beta = np.nanmin(below_cbh_beta)\n",
    "                    dumdiff = cbh_beta - min_below_cbh_beta\n",
    "                    min_diff_arr_out[tt] = dumdiff\n",
    "                    if cbh_beta < -4.5:\n",
    "                        continue\n",
    "                    if np.isnan(dumdiff):\n",
    "                        continue\n",
    "                    elif (~np.isnan(dumdiff)) & (dumdiff >= 1.):\n",
    "                        continue\n",
    "                    elif (~np.isnan(dumdiff)) & (dumdiff < 1.):\n",
    "                        fog_mask[tt] = 1\n",
    "                else:\n",
    "                    min_diff_arr_out[tt] = np.nan\n",
    "            #print(aaaa)\n",
    "            # Shade fog periods\n",
    "            # Fill in fog time periods with transparent blue\n",
    "            fog_id = np.where(fog_mask == 1.)\n",
    "            if np.size(fog_id) > 1.:\n",
    "                fog_Objects,num_fog_objects = ndimage.label(fog_mask)\n",
    "                for kk in range(num_fog_objects):\n",
    "                    dumid = np.where(fog_Objects == kk+1)[0]\n",
    "                    first_id = dumid[0]\n",
    "                    last_id = dumid[-1]\n",
    "                    ax_interp_nn.axvspan(basta_time_dt[first_id],\\\n",
    "                                basta_time_dt[last_id],color='navy',alpha=0.5)               \n",
    "                   \n",
    "            \n",
    "            blue_patch = mpatches.Patch(color='navy',alpha=0.5,label='Fog')\n",
    "            lgnd2 = ax_interp_nn.legend(handles=[blue_patch],\\\n",
    "                                fontsize=Fontsize*1.5,\\\n",
    "                                bbox_to_anchor=(0,1.2),\\\n",
    "                                ncol=1,loc='upper left',framealpha=0)                        \n",
    "            \n",
    "            \n",
    "            ax_interp_nn.add_artist(lgnd)\n",
    "            plt.subplots_adjust(hspace=0.3,wspace=0.1)\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "          \n",
    "            \n",
    "        #================================================\n",
    "        # Interpolate backscatter to radar grid\n",
    "        #================================================            \n",
    "        if interp_backscatter:\n",
    "            basta_time_dt = np.array([toDatetime(basta_time_ts[dd]) for dd in range(len(basta_time_ts))])             \n",
    "            start_time = datetime.datetime(basta_time_dt[0].year,basta_time_dt[0].month,basta_time_dt[0].day,0,0)\n",
    "            end_time = start_time + datetime.timedelta(hours=24)          \n",
    "\n",
    "            ceil_time_dt = np.array(ceil_time_dt)\n",
    "            ceil_time_dt_orig = ceil_time_dt.copy()\n",
    "            ceil_time_ts_orig = ceil_time_ts.copy()\n",
    "            ceil_backscatter_orig = ceil_backscatter.copy()\n",
    "\n",
    "            dumid = np.where( (ceil_time_ts >= basta_time_ts[0]) & (ceil_time_ts <= basta_time_ts[-1]))\n",
    "            if np.size(dumid) > 0.:\n",
    "                dumid = np.squeeze(dumid)\n",
    "                ceil_time_ts = ceil_time_ts[dumid]\n",
    "                ceil_time_dt = ceil_time_dt[dumid]\n",
    "                ceil_backscatter = ceil_backscatter[dumid,:]        \n",
    "\n",
    "            # Interpolated in log10 space (nearest neighbor)\n",
    "            x=ceil_time_ts = np.array([toTimestamp(ceil_time_dt[dd]) for dd in range(len(ceil_time_dt))])\n",
    "            y=ceil_height\n",
    "            #mask invalid values\n",
    "            z = ceil_backscatter.copy()\n",
    "            z = z.T\n",
    "            xx, yy = np.meshgrid(x, y)\n",
    "            #basta_height_lim = basta_height[basta_height < np.max(ceil_range)]\n",
    "            #newy = basta_height_lim\n",
    "            newy = basta_height\n",
    "            newx = basta_time_ts\n",
    "            newX,newY = np.meshgrid(newx,newy)\n",
    "            ceil_backscatter_interp = griddata((xx.ravel(), yy.ravel()), z.ravel(),(newX, newY),method='nearest',fill_value=np.nan)\n",
    "\n",
    "            dumid = np.where( (basta_time_ts < ceil_time_ts_orig[0]) | (basta_time_ts > ceil_time_ts_orig[-1]) )\n",
    "            if np.size(dumid) > 0.:\n",
    "                dumid = np.squeeze(dumid)\n",
    "                ceil_cbh_1_interp[dumid] = np.nan\n",
    "                ceil_cbh_2_interp[dumid] = np.nan\n",
    "                ceil_cbh_3_interp[dumid] = np.nan\n",
    "                ceil_backscatter_interp[:,dumid] = np.nan\n",
    "                ceil_detection_status_interp[dumid] = np.nan\n",
    "                ceil_cbh_bin_relative_interp[dumid] = np.nan\n",
    "\n",
    "            # Need to deal with missing times in between the start and end time of the radar\n",
    "            # Let's loop through basta times and if the time is not within 16 seconds of a\n",
    "            # radar profile, we'll nan it out\n",
    "            for dum_tt in range(len(basta_time_dt)):\n",
    "                dum_diff = np.abs(basta_time_ts[dum_tt] - ceil_time_ts)\n",
    "                if np.min(dum_diff) > 16.:\n",
    "                    ceil_backscatter_interp[:,dum_tt] = np.nan\n",
    "        else:\n",
    "            ceil_backscatter_interp = None\n",
    "            basta_height_lim = None            \n",
    "                  \n",
    "        \n",
    "        aad_ceil_out_dict = {'cbh_1':ceil_cbh_1_interp,\\\n",
    "                    'cbh_2':ceil_cbh_2_interp,\\\n",
    "                    'cbh_3':ceil_cbh_3_interp,\\\n",
    "                    'detection_status':ceil_detection_status_interp,\\\n",
    "                    'backscatter':ceil_backscatter_interp,\\\n",
    "                    'native_cbh_1':ceil_cbh_1,\\\n",
    "                    'native_cbh_2':ceil_cbh_2,\\\n",
    "                    'native_cbh_3':ceil_cbh_3,\\\n",
    "                    'native_detection_status':ceil_detection_status,\\\n",
    "                    'native_backscatter':ceil_backscatter_orig,\\\n",
    "                    'native_time_dt':ceil_time_dt_orig,\\\n",
    "                    'native_time_ts':ceil_time_ts_orig,\\\n",
    "                    'native_range':ceil_height,\\\n",
    "                    'interp_height':basta_height,\\\n",
    "                    'cbh_bin_relative_interp':ceil_cbh_bin_relative_interp,\\\n",
    "                   }\n",
    "     \n",
    "        return aad_ceil_present_flag,aad_ceil_out_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ec0d6b3f-d078-4fed-978d-850eafa9fab7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "Date:  2017/01/27\n",
      "-------------------------------------------------\n",
      "-------------------------------------------------\n",
      "# of current soundings: 3\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "start_index = 290\n",
    "end_index = len(basta_dates_dt)\n",
    "props_dict = {'basta_files':basta_files,\\\n",
    "         'basta_dates_dt':basta_dates_dt,\\\n",
    "         'arm_ceil_files':arm_ceil_files,\\\n",
    "         'arm_ceil_dates_dt':arm_ceil_dates_dt,\\\n",
    "         'aad_ceil_files':aad_ceil_files,\\\n",
    "         'aad_ceil_dates_dt':aad_ceil_dates_dt,\\\n",
    "         'sonde_files':sonde_files,\\\n",
    "         'sonde_dates_dt':sonde_dates_dt,\\\n",
    "         'sonde_times_dt':sonde_times_dt,\\\n",
    "         'cluster_ids':cluster_ids,\\\n",
    "         'cluster_times_dt':cluster_times_dt}\n",
    "props_dict_mult = []\n",
    "for jj in range(len(basta_dates_dt[start_index:end_index])):\n",
    "    props_dict_mult.append(props_dict)\n",
    "#futures = client.map(merge_instruments,basta_dates_dt[start_index:end_index],props_dict_mult)\n",
    "merge_instruments(basta_dates_dt[297],props_dict)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6a561-6316-4407-8c76-91f4e770fa3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02074eb-26c8-46d6-a8ea-7fd60a2c76ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f12806-3491-4b14-8640-62136f275351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364e4d1f-3575-4d43-8e5c-30c04386b4f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629a145a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca8ffbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67c86f1-81d7-4887-951b-957325382c3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3aeb2e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    props_dict = {'basta_files':basta_files,\\\n",
    "             'basta_dates_dt':basta_dates_dt,\\\n",
    "             'arm_ceil_files':arm_ceil_files,\\\n",
    "             'arm_ceil_dates_dt':arm_ceil_dates_dt,\\\n",
    "             'aad_ceil_files':aad_ceil_files,\\\n",
    "             'aad_ceil_dates_dt':aad_ceil_dates_dt,\\\n",
    "             'sonde_files':sonde_files,\\\n",
    "             'sonde_dates_dt':sonde_dates_dt,\\\n",
    "             'sonde_times_dt':sonde_times_dt,\\\n",
    "             'cluster_ids':cluster_ids,\\\n",
    "             'cluster_times_dt':cluster_times_dt}\n",
    "\n",
    "    futures = []\n",
    "    for jj in range(len(basta_dates_dt[0:8])):\n",
    "        print(jj)\n",
    "        future = client.submit(merge_instruments,basta_dates_dt[jj],props_dict)\n",
    "        futures.append(futures)\n",
    "    #result = client.gather(futures)\n",
    "    #print(type(result))\n",
    "    #merge_instruments(basta_dates_dt[0],props_dict)\n",
    "    #print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "316a848e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster = LocalCluster(n_workers=8,threads_per_worker=1,memory_limit='8GB',processes=True)\n",
    "#cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b02a376-3724-4411-8514-06c019e1386d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#client = Client(cluster)\n",
    "#client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d10a1d5-44b9-4b7d-a57a-61007db2cf8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68208d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "\n",
    "def trim_memory() -> int:\n",
    "    libc = ctypes.CDLL(\"libc.so.6\")\n",
    "    return libc.malloc_trim(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183eeb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.run(trim_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec450bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
